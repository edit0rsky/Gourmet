{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Data Preparation Pipeline\n",
    "최종 산출물 : `review_business_5up_5aspect_3sentiment_vectorized_clean.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb910ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import DebertaV2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1139d5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 6855개 항목이 'data/output/business.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: business.json preprocessing ---\n",
    "df_B = pd.read_json(\"data/raw/yelp_academic_dataset_business.json\", lines=True)\n",
    "business_df = df_B.copy()\n",
    "drop_cols = ['postal_code','latitude','longitude','attributes','hours']\n",
    "business_df = business_df.drop(columns=drop_cols)\n",
    "business_df.loc[business_df['city'].str.lower().str.contains(\"philadelphia\", na=False),'city'] = \"Philadelphia\"\n",
    "\n",
    "def load_categories(fp):\n",
    "    with open(fp,'r',encoding='utf-8') as f:\n",
    "        return set(line.strip().lower() for line in f if line.strip())\n",
    "food_categories = load_categories('data/raw/food.txt')\n",
    "restaurant_categories = load_categories('data/raw/restaurant.txt')\n",
    "target_categories = food_categories.union(restaurant_categories)\n",
    "\n",
    "def category_match(row):\n",
    "    if isinstance(row,str):\n",
    "        biz_categories = set(cat.strip().lower() for cat in row.split(','))\n",
    "        return bool(biz_categories & target_categories)\n",
    "    return False\n",
    "business_food_df = business_df[business_df['categories'].apply(category_match)]\n",
    "top_state = business_food_df['state'].value_counts().idxmax()\n",
    "business_pa_df = business_food_df[business_food_df['state']==top_state]\n",
    "business_paph_df = business_pa_df[business_pa_df['city']==\"Philadelphia\"]\n",
    "mask = business_paph_df.apply(lambda col: col.map(lambda x: pd.isna(x) or (isinstance(x,str) and x.strip()==\"\"))).any(axis=1)\n",
    "business_paph_df_2 = business_paph_df[~mask].reset_index(drop=True)\n",
    "business_paph_df_2.to_json(\"data/output/business.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "print(f\"총 {len(business_paph_df_2)}개 항목이 'data/output/business.json'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef3881c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730552/730552 [02:22<00:00, 5126.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: review.json preprocessing ---\n",
    "chunk_size=100000\n",
    "chunks=pd.read_json(\"data/raw/yelp_academic_dataset_review.json\", lines=True, chunksize=chunk_size)\n",
    "df_review=pd.concat(chunk for chunk in chunks)\n",
    "business_ids=set(business_paph_df_2['business_id'])\n",
    "df_review=df_review[df_review['business_id'].isin(business_ids)]\n",
    "df_review=df_review.drop(columns=['funny','cool'])\n",
    "\n",
    "tqdm.pandas()\n",
    "tokenizer=DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "df_review['token_length']=df_review['text'].progress_apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df_review.to_json(\"data/output/review.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc72f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: user.json preprocessing ---\n",
    "chunks=pd.read_json(\"data/raw/yelp_academic_dataset_user.json\", lines=True, chunksize=100000)\n",
    "df_user=pd.concat(chunk for chunk in chunks)\n",
    "drop_columns=['yelping_since','funny','cool','elite','friends','fans','compliment_hot','compliment_more','compliment_profile','compliment_cute','compliment_list','compliment_note','compliment_plain','compliment_cool','compliment_funny','compliment_writer','compliment_photos']\n",
    "df_user=df_user.drop(columns=drop_columns)\n",
    "review_counts=df_review['user_id'].value_counts()\n",
    "user_ids_5plus=review_counts[review_counts>=5].index\n",
    "df_user=df_user[df_user['user_id'].isin(user_ids_5plus)]\n",
    "df_user.to_json(\"data/output/user.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38317da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "병합 중: 100%|██████████| 730552/730552 [00:02<00:00, 317848.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "병합 완료: merged_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: merge review, user and business ---\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "reviews = load_jsonl(\"data/output/review.json\")\n",
    "users = load_jsonl(\"data/output/user.json\")\n",
    "businesses = load_jsonl(\"data/output/business.json\")\n",
    "\n",
    "user_dict = {u['user_id']: u for u in users}\n",
    "business_dict = {b['business_id']: b for b in businesses}\n",
    "\n",
    "merged_data = []\n",
    "for r in tqdm(reviews, desc='병합 중'):\n",
    "    uid = r['user_id']\n",
    "    bid = r['business_id']\n",
    "    if uid in user_dict and bid in business_dict:\n",
    "        m = r.copy()\n",
    "        for k, v in user_dict[uid].items():\n",
    "            m[f'user_{k}'] = v\n",
    "        for k, v in business_dict[bid].items():\n",
    "            m[f'business_{k}'] = v\n",
    "        merged_data.append(m)\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df = pd.DataFrame(merged_data)\n",
    "# 불필요한 ID 컬럼 제거\n",
    "df.drop(columns=[\"user_user_id\", \"business_business_id\"], inplace=True, errors=\"ignore\")\n",
    "# 컬럼 이름 리네이밍\n",
    "df.rename(columns={\n",
    "    \"stars\": \"review_stars\",\n",
    "    \"useful\": \"review_useful\",\n",
    "    \"date\": \"review_date\"}, inplace=True)\n",
    "# 저장 경로\n",
    "output_path = \"data/output/merged_dataset.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "# JSONL 형식으로 저장\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df.to_dict(orient=\"records\"):\n",
    "        json.dump(row, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"병합 완료: merged_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2.5: ABSA 감성 분석 수행 (DeBERTa v3 기반) ---\n",
    "import json, os, torch, psutil, GPUtil\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "# 메모리 상태 출력 함수\n",
    "def print_memory_status():\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            gpu = GPUtil.getGPUs()[0]\n",
    "            print(f\"\\n[GPU 메모리] {gpu.memoryUsed}MB / {gpu.memoryTotal}MB | 사용률: {gpu.memoryUtil*100:.1f}% | 온도: {gpu.temperature}°C\")\n",
    "            print(f\"[PyTorch 메모리] 할당: {torch.cuda.memory_allocated()/1024**2:.1f}MB / 캐시: {torch.cuda.memory_reserved()/1024**2:.1f}MB\")\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"[시스템 메모리] {memory.used/1024**2:.1f}MB / {memory.total/1024**2:.1f}MB | 사용률: {memory.percent}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"메모리 확인 오류: {e}\")\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "MODEL_NAME = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "\n",
    "# 분석할 측면\n",
    "ASPECTS = [\"food\", \"service\", \"price\", \"ambience\", \"location\"]\n",
    "\n",
    "# 감성 분석 함수\n",
    "def analyze_sentiment(text, aspect):\n",
    "    try:\n",
    "        inputs = tokenizer(text, aspect, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = torch.softmax(logits, dim=1)[0]\n",
    "        return {\"scores\": {model.config.id2label[i]: float(probs[i]) for i in range(len(probs))}, \"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e} (텍스트: {text[:50]}...)\")\n",
    "        return {\"scores\": None, \"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_file = \"data/output/merged_dataset.json\"\n",
    "output_file = \"data/output/review_5up_5aspect_3sentiment.jsonl\"\n",
    "\n",
    "# 기존 처리 리뷰 ID 불러오기\n",
    "processed_ids = set()\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                processed_ids.add(obj[\"review_id\"])\n",
    "            except:\n",
    "                continue\n",
    "    print(f\"{len(processed_ids)}개 리뷰는 건너뜁니다.\")\n",
    "\n",
    "# 리뷰 처리 및 저장\n",
    "buffer, count, skipped = [], 0, 0\n",
    "save_every = 500\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in tqdm(fin, desc=\"ABSA 감성 분석 중\"):\n",
    "        try:\n",
    "            obj = json.loads(line.strip())\n",
    "            rid, text = obj.get(\"review_id\"), obj.get(\"text\", \"\")\n",
    "            if not text or rid in processed_ids:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            sentiment = {asp: analyze_sentiment(text, asp) for asp in ASPECTS}\n",
    "            buffer.append({\"review_id\": rid, \"text\": text, \"sentiment\": sentiment})\n",
    "            count += 1\n",
    "            if len(buffer) >= save_every:\n",
    "                with open(output_file, \"a\", encoding=\"utf-8\") as fout:\n",
    "                    for r in buffer:\n",
    "                        fout.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "                print(f\"{count}개 리뷰 저장됨\")\n",
    "                print_memory_status()\n",
    "                buffer = []\n",
    "        except Exception as e:\n",
    "            print(f\"리뷰 처리 중 오류 발생: {e}\")\n",
    "            continue\n",
    "\n",
    "# 남은 리뷰 저장\n",
    "if buffer:\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as fout:\n",
    "        for r in buffer:\n",
    "            fout.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"총 {count}개 리뷰 분석 완료, {skipped}개 건너뜀 → {output_file}\")\n",
    "print_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65227fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 완료: 총 451,185건 중 리뷰 5개 이상 유저 리뷰 451,185건 유지\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "최종 벡터화 및 필터링: 452505it [00:09, 45375.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "총 결과 리뷰 수: 447,796건 저장 완료 → data/output/review_business_5up_5aspect_3sentiment_vectorized_clean.json\n",
      "- 병합되지 않은 리뷰: 1,320건\n",
      "- business 방문 수 5개 미만으로 제외된 리뷰: 3,389건\n",
      "- JSON 디코딩 실패: 0건\n"
     ]
    }
   ],
   "source": [
    "# --- 입력 파일 경로 ---\n",
    "merged_path = \"data/output/merged_dataset.json\"\n",
    "absa_path = \"data/output/review_5up_5aspect_3sentiment.jsonl\"\n",
    "final_output_path = \"data/output/review_business_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "\n",
    "# --- Step 1: 5개 이상 리뷰 작성 유저 필터링 ---\n",
    "user_review_counts = defaultdict(int)\n",
    "merged_data = []\n",
    "with open(merged_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        user_review_counts[obj[\"user_id\"]] += 1\n",
    "        merged_data.append(obj)\n",
    "\n",
    "qualified_users = {u for u, c in user_review_counts.items() if c >= 5}\n",
    "merged_filtered = [obj for obj in merged_data if obj[\"user_id\"] in qualified_users]\n",
    "print(f\"Step 1 완료: 총 {len(merged_data):,}건 중 리뷰 5개 이상 유저 리뷰 {len(merged_filtered):,}건 유지\")\n",
    "\n",
    "# --- Step 2: review_id → user 정보 맵 생성 ---\n",
    "id_map = {}\n",
    "user_biz_ids = defaultdict(set)\n",
    "\n",
    "for obj in merged_filtered:\n",
    "    rid = obj[\"review_id\"]\n",
    "    uid = obj[\"user_id\"]\n",
    "    bid = obj[\"business_id\"]\n",
    "    id_map[rid] = {\n",
    "        \"user_id\": uid,\n",
    "        \"business_id\": bid,\n",
    "        \"stars\": obj[\"review_stars\"],\n",
    "        \"review_useful\": obj[\"review_useful\"],\n",
    "        \"review_date\": obj[\"review_date\"],\n",
    "    }\n",
    "    user_biz_ids[uid].add(bid)\n",
    "\n",
    "# --- Step 3: ABSA 결과 결합 + 벡터화 + 최종 필터링 ---\n",
    "def sentiment_to_vector(sentiment_dict):\n",
    "    aspects = [\"food\", \"service\", \"price\", \"ambience\", \"location\"]\n",
    "    polarities = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "    vector = []\n",
    "    for asp in aspects:\n",
    "        scores = sentiment_dict.get(asp, {}).get(\"scores\", {})\n",
    "        for pol in polarities:\n",
    "            vector.append(scores.get(pol, 0.0))\n",
    "    return vector\n",
    "\n",
    "final_data = []\n",
    "not_in_merged = 0\n",
    "too_few_businesses = 0\n",
    "decode_errors = 0\n",
    "\n",
    "with open(absa_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"최종 벡터화 및 필터링\"):\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            rid = obj.get(\"review_id\")\n",
    "            if rid not in id_map:\n",
    "                not_in_merged += 1\n",
    "                continue\n",
    "            info = id_map[rid]\n",
    "            uid = info[\"user_id\"]\n",
    "            if len(user_biz_ids[uid]) < 5:\n",
    "                too_few_businesses += 1\n",
    "                continue\n",
    "            vec = sentiment_to_vector(obj.get(\"sentiment\", {}))\n",
    "            final_data.append({\n",
    "                \"review_id\": rid,\n",
    "                \"user_id\": uid,\n",
    "                \"business_id\": info[\"business_id\"],\n",
    "                \"stars\": info[\"stars\"],\n",
    "                \"review_date\": info[\"review_date\"],\n",
    "                \"sentiment_vector\": vec,\n",
    "            })\n",
    "        except json.JSONDecodeError:\n",
    "            decode_errors += 1\n",
    "            continue\n",
    "\n",
    "# --- 최종 저장 ---\n",
    "with open(final_output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for obj in final_data:\n",
    "        fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# --- 통계 출력 ---\n",
    "print(f\"\\n총 결과 리뷰 수: {len(final_data):,}건 저장 완료 → {final_output_path}\")\n",
    "print(f\"- 병합되지 않은 리뷰: {not_in_merged:,}건\")\n",
    "print(f\"- business 방문 수 5개 미만으로 제외된 리뷰: {too_few_businesses:,}건\")\n",
    "print(f\"- JSON 디코딩 실패: {decode_errors:,}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f87e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/output/review_business_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "absa_path = \"data/output/review_5up_5aspect_3sentiment.jsonl\"\n",
    "output_path = \"data/output/review_business_5up_with_text.json\"\n",
    "\n",
    "review_text_map = {}\n",
    "with open(absa_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            review_text_map[obj[\"review_id\"]] = obj.get(\"text\", \"\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# 입력 파일 읽어서 text 추가, sentiment_vector 제거\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fin, open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in tqdm(fin, desc=\"텍스트 추가 중\"):\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            rid = obj[\"review_id\"]\n",
    "            obj[\"text\"] = review_text_map.get(rid, \"\")\n",
    "            obj.pop(\"sentiment_vector\", None)\n",
    "            fout.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"처리 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"완료: 텍스트 포함 리뷰가 '{output_path}'에 저장\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
