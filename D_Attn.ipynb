{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-uHYkB1H-6Y",
        "outputId": "02d3fc6d-1190-4b9e-8f82-d373dac164d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ 2. 라이브러리 임포트 ------------------\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ------------------ 3. GloVe 로딩 ------------------\n",
        "def load_glove_embeddings(glove_path, embedding_dim=100):\n",
        "    embeddings = {}\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    return text.split()\n",
        "\n",
        "def text_to_glove_sequence(text, glove_embeddings, max_len=300, embed_dim=100):\n",
        "    tokens = preprocess(text)\n",
        "    vectors = [glove_embeddings[token] for token in tokens if token in glove_embeddings]\n",
        "    if len(vectors) < max_len:\n",
        "        vectors.extend([np.zeros(embed_dim)] * (max_len - len(vectors)))\n",
        "    else:\n",
        "        vectors = vectors[:max_len]\n",
        "    return np.stack(vectors)\n",
        "\n",
        "# ------------------ 4. 데이터 로딩 + 리뷰 임베딩 ------------------\n",
        "data = []\n",
        "with open('/content/drive/MyDrive/review_business_5up_with_text.json', 'r') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Loading GloVe...\")\n",
        "glove_path = '/content/drive/MyDrive/glove.6B.100d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "print(\"Embedding review texts...\")\n",
        "df['glove_sequence'] = [text_to_glove_sequence(t, glove_embeddings) for t in tqdm(df['text'])]\n",
        "\n",
        "user_group = df.groupby('user_id')['glove_sequence'].apply(lambda x: np.mean(np.stack(x), axis=0))\n",
        "item_group = df.groupby('business_id')['glove_sequence'].apply(lambda x: np.mean(np.stack(x), axis=0))\n",
        "\n",
        "user2idx = {u: i for i, u in enumerate(user_group.index)}\n",
        "item2idx = {b: i for i, b in enumerate(item_group.index)}\n",
        "df = df[df['user_id'].isin(user2idx) & df['business_id'].isin(item2idx)]\n",
        "\n",
        "user_embeddings = np.stack([user_group[uid] for uid in df['user_id']])\n",
        "item_embeddings = np.stack([item_group[iid] for iid in df['business_id']])\n",
        "ratings = df['stars'].values\n",
        "\n",
        "# ------------------ 5. Dataset 정의 ------------------\n",
        "class DAttnDataset(Dataset):\n",
        "    def __init__(self, user_reviews, item_reviews, ratings):\n",
        "        self.user_reviews = user_reviews\n",
        "        self.item_reviews = item_reviews\n",
        "        self.ratings = ratings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ratings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.user_reviews[idx], dtype=torch.float32),\n",
        "            torch.tensor(self.item_reviews[idx], dtype=torch.float32),\n",
        "            torch.tensor(self.ratings[idx], dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "# ------------------ 6. 모델 정의 ------------------\n",
        "class LocalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=100, num_filters=200, filter_size=3):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(embed_dim, num_filters, filter_size, padding=filter_size // 2)\n",
        "        self.attn_fc = nn.Linear(num_filters, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        conv_out = torch.tanh(self.conv(x))\n",
        "        conv_out = conv_out.transpose(1, 2)\n",
        "        attn_scores = self.attn_fc(conv_out)\n",
        "        attn_weights = F.softmax(attn_scores, dim=1)\n",
        "        weighted = torch.sum(conv_out * attn_weights, dim=1)\n",
        "        return weighted\n",
        "\n",
        "class GlobalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=100, num_filters=100, filter_sizes=[2, 3, 4]):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(embed_dim, num_filters, fs) for fs in filter_sizes\n",
        "        ])\n",
        "        self.attn_fc = nn.Linear(num_filters * len(filter_sizes), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            c = torch.tanh(conv(x))\n",
        "            c = F.max_pool1d(c, kernel_size=c.shape[2])\n",
        "            conv_outs.append(c.squeeze(2))\n",
        "        merged = torch.cat(conv_outs, dim=1)\n",
        "        attn_weights = F.softmax(self.attn_fc(merged), dim=1)\n",
        "        weighted = merged * attn_weights\n",
        "        return weighted\n",
        "\n",
        "class DAttnRecommender(nn.Module):\n",
        "    def __init__(self, embed_dim=100):\n",
        "        super().__init__()\n",
        "        self.user_lattn = LocalAttention(embed_dim)\n",
        "        self.item_lattn = LocalAttention(embed_dim)\n",
        "        self.user_gattn = GlobalAttention(embed_dim)\n",
        "        self.item_gattn = GlobalAttention(embed_dim)\n",
        "\n",
        "        feature_dim = (200 + 100 * 3) * 2\n",
        "        self.fc1 = nn.Linear(feature_dim, 500)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(500, 50)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.output = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, user_reviews, item_reviews):\n",
        "        u_l = self.user_lattn(user_reviews)\n",
        "        i_l = self.item_lattn(item_reviews)\n",
        "        u_g = self.user_gattn(user_reviews)\n",
        "        i_g = self.item_gattn(item_reviews)\n",
        "        x = torch.cat([u_l, u_g, i_l, i_g], dim=1)\n",
        "        x = self.dropout1(F.relu(self.fc1(x)))\n",
        "        x = self.dropout2(F.relu(self.fc2(x)))\n",
        "        return self.output(x).squeeze(1)\n",
        "\n",
        "# ------------------ 7. 학습 및 평가 함수 ------------------\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for u, i, r in dataloader:\n",
        "            u, i, r = u.to(device), i.to(device), r.to(device)\n",
        "            o = model(u, i)\n",
        "            preds.extend(o.cpu().numpy())\n",
        "            targets.extend(r.cpu().numpy())\n",
        "    preds, targets = np.array(preds), np.array(targets)\n",
        "    mse = mean_squared_error(targets, preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(targets, preds)\n",
        "    mape = np.mean(np.abs((targets - preds) / (targets + 1e-10))) * 100\n",
        "    print(f\"\\n✅ [D-Attn] 최종 테스트 평가 지표:\\n   - MSE  : {mse:.4f}\\n   - RMSE : {rmse:.4f}\\n   - MAE  : {mae:.4f}\\n   - MAPE : {mape:.2f}%\")\n",
        "    return mse, rmse, mae, mape\n",
        "\n",
        "def train_dattn(model, train_loader, val_loader, device, lr=1e-3, epochs=50, patience=5, min_delta=0.001):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    best_val_rmse = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    model_path = 'best_dattn_model.pt'\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for u, i, r in train_loader:\n",
        "            u, i, r = u.to(device), i.to(device), r.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(u, i)\n",
        "            loss = criterion(output, r)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # 검증\n",
        "        model.eval()\n",
        "        val_preds, val_true = [], []\n",
        "        with torch.no_grad():\n",
        "            for u, i, r in val_loader:\n",
        "                u, i, r = u.to(device), i.to(device), r.to(device)\n",
        "                output = model(u, i)\n",
        "                val_preds.extend(output.cpu().numpy())\n",
        "                val_true.extend(r.cpu().numpy())\n",
        "\n",
        "        val_rmse = np.sqrt(mean_squared_error(val_true, val_preds))\n",
        "        val_mae = mean_absolute_error(val_true, val_preds)\n",
        "        val_mape = np.mean(np.abs((np.array(val_true) - np.array(val_preds)) / (np.array(val_true) + 1e-10))) * 100\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} | Train Loss: {total_loss/len(train_loader):.4f} | \"\n",
        "              f\"Val RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, MAPE: {val_mape:.2f}%\")\n",
        "\n",
        "        if val_rmse < best_val_rmse - min_delta:\n",
        "            best_val_rmse = val_rmse\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(f\"  --> 개선됨. 모델 저장됨 (RMSE: {best_val_rmse:.4f})\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  --> 개선 없음. ({epochs_no_improve}/{patience})\")\n",
        "            if epochs_no_improve == patience:\n",
        "                print(\"조기 종료 발생.\")\n",
        "                break\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"최적 모델 로드 완료: {model_path}\")\n",
        "    return model\n",
        "\n",
        "# ------------------ 8. 데이터 분할 및 학습 실행 ------------------\n",
        "X_user_trainval, X_user_test, X_item_trainval, X_item_test, y_trainval, y_test = train_test_split(\n",
        "    user_embeddings, item_embeddings, ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "X_user_train, X_user_val, X_item_train, X_item_val, y_train, y_val = train_test_split(\n",
        "    X_user_trainval, X_item_trainval, y_trainval, test_size=0.125, random_state=42)\n",
        "\n",
        "train_dataset = DAttnDataset(X_user_train, X_item_train, y_train)\n",
        "val_dataset = DAttnDataset(X_user_val, X_item_val, y_val)\n",
        "test_dataset = DAttnDataset(X_user_test, X_item_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DAttnRecommender()\n",
        "model = train_dattn(model, train_loader, val_loader, device)\n",
        "evaluate_model(model, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yILlzrN4INoV",
        "outputId": "433b9d42-d907-46fb-db16-4187782a60e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe...\n",
            "Embedding review texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 46031/447796 [00:43<03:34, 1869.83it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23232"
      ],
      "metadata": {
        "id": "J0_4A0wrZvBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# GloVe 로딩 함수\n",
        "def load_glove_embeddings(glove_path, embedding_dim=100):\n",
        "    embeddings = {}\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# 전처리 + GloVe 임베딩 함수\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    return text.split()\n",
        "\n",
        "def text_to_glove_sequence(text, glove_embeddings, max_len=300, embed_dim=100):\n",
        "    tokens = preprocess(text)\n",
        "    vectors = [glove_embeddings[token] for token in tokens if token in glove_embeddings]\n",
        "    if len(vectors) < max_len:\n",
        "        vectors.extend([np.zeros(embed_dim)] * (max_len - len(vectors)))\n",
        "    else:\n",
        "        vectors = vectors[:max_len]\n",
        "    return np.stack(vectors).astype(np.float16)\n",
        "\n",
        "# 경로 설정\n",
        "json_path = '/content/drive/MyDrive/review_business_5up_with_text.json'\n",
        "glove_path = '/content/drive/MyDrive/glove.6B.100d.txt'\n",
        "save_dir = '/content/drive/MyDrive/glove_chunks'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# GloVe 임베딩 로드\n",
        "print(\"Loading GloVe...\")\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "# 리뷰 JSON 라인 단위로 읽고 처리\n",
        "chunk_size = 10000\n",
        "chunk_idx = 0\n",
        "buffer = []\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    for line_num, line in enumerate(f):\n",
        "        buffer.append(json.loads(line))\n",
        "        if len(buffer) == chunk_size:\n",
        "            df = pd.DataFrame(buffer)\n",
        "            print(f\"[Chunk {chunk_idx}] Embedding {len(df)} reviews...\")\n",
        "            glove_seqs = [text_to_glove_sequence(t, glove_embeddings) for t in tqdm(df['text'])]\n",
        "            np.save(os.path.join(save_dir, f'glove_chunk_{chunk_idx}.npy'), glove_seqs)\n",
        "            df[['user_id', 'business_id', 'stars']].to_csv(os.path.join(save_dir, f'meta_chunk_{chunk_idx}.csv'), index=False)\n",
        "            buffer = []\n",
        "            chunk_idx += 1\n",
        "\n",
        "# 마지막 남은 리뷰 처리\n",
        "if buffer:\n",
        "    df = pd.DataFrame(buffer)\n",
        "    print(f\"[Chunk {chunk_idx}] Embedding {len(df)} reviews...\")\n",
        "    glove_seqs = [text_to_glove_sequence(t, glove_embeddings) for t in tqdm(df['text'])]\n",
        "    np.save(os.path.join(save_dir, f'glove_chunk_{chunk_idx}.npy'), glove_seqs)\n",
        "    df[['user_id', 'business_id', 'stars']].to_csv(os.path.join(save_dir, f'meta_chunk_{chunk_idx}.csv'), index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7pklrNGZuh4",
        "outputId": "32cae3e8-39b6-47e5-a6d3-81e6ded6bfba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe...\n",
            "[Chunk 0] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1463.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 1] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1536.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 2] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:18<00:00, 533.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 3] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1417.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 4] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1325.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 5] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1373.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 6] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:10<00:00, 977.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 7] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1563.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 8] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1258.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 9] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1468.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 10] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:08<00:00, 1140.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 11] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1253.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 12] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1332.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 13] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1306.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 14] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1405.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 15] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:11<00:00, 894.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 16] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1402.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 17] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:11<00:00, 897.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 18] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1665.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 19] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:10<00:00, 916.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 20] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1894.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 21] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1408.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 22] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1426.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 23] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1886.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 24] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1428.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 25] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1422.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 26] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1852.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 27] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1902.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 28] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1423.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 29] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1395.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 30] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1512.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 31] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1405.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 32] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1889.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 33] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1887.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 34] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1867.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 35] Embedding 10000 reviews...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:05<00:00, 1828.27it/s]\n"
          ]
        }
      ]
    }
  ]
}