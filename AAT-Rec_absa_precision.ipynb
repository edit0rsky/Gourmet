{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfa6067-da26-478b-a526-3c6216a64755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 수: 447796\n",
      "학습 데이터 수: 313456 (70.00%)\n",
      "검증 데이터 수: 44780 (10.00%)\n",
      "테스트 데이터 수: 89560 (20.00%)\n",
      "모델 학습 시작...\n",
      "Epoch 1/50, Train Loss: 0.7677, Val Loss: 0.5071, Val RMSE: 0.7121\n",
      "  --> 검증 RMSE 개선됨. 최적 모델 저장됨: 0.7121\n",
      "Epoch 2/50, Train Loss: 0.4822, Val Loss: 0.4881, Val RMSE: 0.6986\n",
      "  --> 검증 RMSE 개선됨. 최적 모델 저장됨: 0.6986\n",
      "Epoch 3/50, Train Loss: 0.4493, Val Loss: 0.4776, Val RMSE: 0.6911\n",
      "  --> 검증 RMSE 개선됨. 최적 모델 저장됨: 0.6911\n",
      "Epoch 4/50, Train Loss: 0.4217, Val Loss: 0.4860, Val RMSE: 0.6972\n",
      "  --> 검증 RMSE 개선되지 않음. 대기 중... (1/5)\n",
      "Epoch 5/50, Train Loss: 0.3975, Val Loss: 0.4928, Val RMSE: 0.7020\n",
      "  --> 검증 RMSE 개선되지 않음. 대기 중... (2/5)\n",
      "Epoch 6/50, Train Loss: 0.3763, Val Loss: 0.4863, Val RMSE: 0.6974\n",
      "  --> 검증 RMSE 개선되지 않음. 대기 중... (3/5)\n",
      "Epoch 7/50, Train Loss: 0.3555, Val Loss: 0.5132, Val RMSE: 0.7164\n",
      "  --> 검증 RMSE 개선되지 않음. 대기 중... (4/5)\n",
      "Epoch 8/50, Train Loss: 0.3345, Val Loss: 0.5020, Val RMSE: 0.7085\n",
      "  --> 검증 RMSE 개선되지 않음. 대기 중... (5/5)\n",
      "조기 종료! 5 에폭 동안 검증 RMSE 개선이 없었습니다.\n",
      "모델 학습 완료.\n",
      "최적의 모델 가중치 'best_aatrec_model.pt' 로드 완료.\n",
      "\n",
      "최종 테스트 RMSE (최적 모델 기준): 0.6883\n",
      "\n",
      "Precision@5 계산 시작...\n",
      "Precision@5 (relevance_threshold=4.0): 0.0001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "# JSONL 파일 로드 (lines=True 필수)\n",
    "# 파일 경로를 실제 파일 경로로 바꿔주세요.\n",
    "df = pd.read_json('review_business_5up_5aspect_3sentiment_vectorized_clean.json', lines=True)\n",
    "\n",
    "# 필요한 컬럼 추출 (여기서 명시적으로 .copy()를 사용하여 새로운 DataFrame을 만듭니다)\n",
    "df_processed = df[['user_id', 'business_id', 'stars', 'sentiment_vector']].copy()\n",
    "\n",
    "# user_id와 business_id를 연속적인 정수 ID로 인코딩\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "# .loc를 사용하여 값을 할당하여 경고를 방지합니다.\n",
    "df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_businesses = len(business_encoder.classes_)\n",
    "\n",
    "# 1. 먼저 학습+검증 세트와 테스트 세트로 8:2 비율로 분할\n",
    "# (7+1):2 = 8:2\n",
    "train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. 학습+검증 세트를 다시 학습 세트와 검증 세트로 7:1 비율로 분할\n",
    "# val_size_ratio는 train_val_df의 1/8이므로, 전체 데이터의 1/10이 됩니다.\n",
    "val_size_ratio = 1 / 8\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size_ratio, random_state=42)\n",
    "\n",
    "print(f\"전체 데이터 수: {len(df_processed)}\")\n",
    "print(f\"학습 데이터 수: {len(train_df)} ({len(train_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"검증 데이터 수: {len(val_df)} ({len(val_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"테스트 데이터 수: {len(test_df)} ({len(test_df)/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "# --- PyTorch Dataset 및 DataLoader 정의 ---\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.user_ids = torch.tensor(df['user_encoded'].values, dtype=torch.long)\n",
    "        self.business_ids = torch.tensor(df['business_encoded'].values, dtype=torch.long)\n",
    "        self.sentiment_vectors = torch.tensor(np.array(df['sentiment_vector'].tolist()), dtype=torch.float)\n",
    "        self.stars = torch.tensor(df['stars'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.business_ids[idx], self.sentiment_vectors[idx], self.stars[idx]\n",
    "\n",
    "# --- 모델 아키텍처 정의 (이전과 동일) ---\n",
    "class CustomerRestaurantInteractionModule(nn.Module):\n",
    "    def __init__(self, num_users, num_businesses, embedding_dim, mlp_dims):\n",
    "        super(CustomerRestaurantInteractionModule, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.business_embedding = nn.Embedding(num_businesses, embedding_dim)\n",
    "        layers = []\n",
    "        input_dim = embedding_dim * 2\n",
    "        for dim in mlp_dims:\n",
    "            layers.append(nn.Linear(input_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = dim\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output_dim = mlp_dims[-1] if mlp_dims else embedding_dim * 2\n",
    "\n",
    "    def forward(self, user_ids, business_ids):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        business_vec = self.business_embedding(business_ids)\n",
    "        combined_vec = torch.cat((user_vec, business_vec), dim=1)\n",
    "        interaction_features = self.mlp(combined_vec)\n",
    "        return interaction_features\n",
    "\n",
    "class ReviewAspectModule(nn.Module):\n",
    "    def __init__(self, sentiment_vector_dim, mlp_dims):\n",
    "        super(ReviewAspectModule, self).__init__()\n",
    "        layers = []\n",
    "        input_dim = sentiment_vector_dim\n",
    "        for dim in mlp_dims:\n",
    "            layers.append(nn.Linear(input_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = dim\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output_dim = mlp_dims[-1] if mlp_dims else sentiment_vector_dim\n",
    "\n",
    "    def forward(self, sentiment_vectors):\n",
    "        aspect_features = self.mlp(sentiment_vectors)\n",
    "        return aspect_features\n",
    "\n",
    "class AATRec(nn.Module):\n",
    "    def __init__(self, num_users, num_businesses, embedding_dim,\n",
    "                 user_biz_mlp_dims, aspect_mlp_dims, final_mlp_dims,\n",
    "                 sentiment_vector_dim=15):\n",
    "        super(AATRec, self).__init__()\n",
    "        self.customer_restaurant_interaction_module = CustomerRestaurantInteractionModule(\n",
    "            num_users, num_businesses, embedding_dim, user_biz_mlp_dims\n",
    "        )\n",
    "        self.review_aspect_module = ReviewAspectModule(\n",
    "            sentiment_vector_dim, aspect_mlp_dims\n",
    "        )\n",
    "        final_input_dim = self.customer_restaurant_interaction_module.output_dim + \\\n",
    "                          self.review_aspect_module.output_dim\n",
    "        layers = []\n",
    "        input_dim = final_input_dim\n",
    "        for dim in final_mlp_dims:\n",
    "            layers.append(nn.Linear(input_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.prediction_mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user_ids, business_ids, sentiment_vectors):\n",
    "        user_biz_features = self.customer_restaurant_interaction_module(user_ids, business_ids)\n",
    "        aspect_features = self.review_aspect_module(sentiment_vectors)\n",
    "        combined_features = torch.cat((user_biz_features, aspect_features), dim=1)\n",
    "        predicted_rating = self.prediction_mlp(combined_features)\n",
    "        # FIX: Ensure the output is always a 1D tensor, not a 0D scalar\n",
    "        return predicted_rating.view(-1) # Use view(-1) to flatten to 1D tensor\n",
    "\n",
    "# --- 하이퍼파라미터 설정 ---\n",
    "embedding_dim = 64\n",
    "user_biz_mlp_dims = [128, 64]\n",
    "aspect_mlp_dims = [64, 32]\n",
    "final_mlp_dims = [64, 32]\n",
    "learning_rate = 0.001\n",
    "epochs = 50 # 조기 종료를 고려하여 충분히 큰 에폭 설정\n",
    "batch_size = 256\n",
    "\n",
    "# --- 조기 종료(Early Stopping) 설정 ---\n",
    "patience = 5 # 검증 RMSE가 개선되지 않아도 기다릴 에폭 수\n",
    "min_delta = 0.001 # 검증 RMSE가 개선되었다고 판단할 최소 변화량\n",
    "best_val_rmse = float('inf') # 초기 최적 검증 RMSE (무한대로 설정)\n",
    "epochs_no_improve = 0 # 검증 RMSE가 개선되지 않은 에폭 수 카운터\n",
    "model_path = 'best_aatrec_model.pt' # 최적 모델 저장 경로\n",
    "\n",
    "# 데이터셋 및 DataLoader 생성\n",
    "train_dataset = ReviewDataset(train_df)\n",
    "val_dataset = ReviewDataset(val_df)\n",
    "test_dataset = ReviewDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = AATRec(num_users, num_businesses, embedding_dim,\n",
    "               user_biz_mlp_dims, aspect_mlp_dims, final_mlp_dims,\n",
    "               sentiment_vector_dim=15)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- 학습 루프 (조기 종료 추가) ---\n",
    "print(\"모델 학습 시작...\")\n",
    "for epoch in range(epochs):\n",
    "    # --- 학습 단계 ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for user_ids, business_ids, sentiment_vectors, stars in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_ids, business_ids, sentiment_vectors)\n",
    "        loss = criterion(predictions, stars)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # --- 검증 단계 ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_true_ratings = []\n",
    "    with torch.no_grad():\n",
    "        for user_ids, business_ids, sentiment_vectors, stars in val_loader:\n",
    "            predictions = model(user_ids, business_ids, sentiment_vectors)\n",
    "            loss = criterion(predictions, stars)\n",
    "            total_val_loss += loss.item()\n",
    "            val_predictions.extend(predictions.tolist())\n",
    "            val_true_ratings.extend(stars.tolist())\n",
    "\n",
    "    current_val_rmse = np.sqrt(mean_squared_error(val_true_ratings, val_predictions))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "          f\"Train Loss: {total_train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Val Loss: {total_val_loss / len(val_loader):.4f}, \"\n",
    "          f\"Val RMSE: {current_val_rmse:.4f}\")\n",
    "\n",
    "    # --- 조기 종료 로직 ---\n",
    "    if current_val_rmse < best_val_rmse - min_delta: # 검증 RMSE가 개선되었다면\n",
    "        best_val_rmse = current_val_rmse\n",
    "        epochs_no_improve = 0 # 개선이 있었으므로 카운트 초기화\n",
    "        torch.save(model.state_dict(), model_path) # 최적 모델 가중치 저장\n",
    "        print(f\"  --> 검증 RMSE 개선됨. 최적 모델 저장됨: {best_val_rmse:.4f}\")\n",
    "    else: # 개선되지 않았다면\n",
    "        epochs_no_improve += 1 # 카운트 증가\n",
    "        print(f\"  --> 검증 RMSE 개선되지 않음. 대기 중... ({epochs_no_improve}/{patience})\")\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"조기 종료! {patience} 에폭 동안 검증 RMSE 개선이 없었습니다.\")\n",
    "            break # 학습 루프 종료\n",
    "\n",
    "print(\"모델 학습 완료.\")\n",
    "\n",
    "# --- 최종 테스트 및 RMSE 계산 ---\n",
    "# 최적의 모델 가중치를 로드하여 테스트\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(f\"최적의 모델 가중치 '{model_path}' 로드 완료.\")\n",
    "else:\n",
    "    print(f\"최적의 모델 가중치 '{model_path}'를 찾을 수 없습니다. 현재 모델 상태로 테스트를 진행합니다.\")\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "true_ratings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for user_ids, business_ids, sentiment_vectors, stars in test_loader:\n",
    "        predictions = model(user_ids, business_ids, sentiment_vectors)\n",
    "        test_predictions.extend(predictions.tolist()) # This should now work correctly\n",
    "        true_ratings.extend(stars.tolist())\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(true_ratings, test_predictions))\n",
    "print(f\"\\n최종 테스트 RMSE (최적 모델 기준): {rmse:.4f}\")\n",
    "\n",
    "# --- Precision@5 계산을 위한 추가 전처리 ---\n",
    "# 1. 각 business_encoded별 평균 sentiment_vector 계산 (ABSA가 없는 아이템에 대한 대안)\n",
    "#    주의: 실제 운영에서는 unseen item에 대한 sentiment_vector를 추론하거나, 해당 item의 평균 리뷰에서 추출해야 함\n",
    "business_sentiment_map = train_df.groupby('business_encoded')['sentiment_vector'].apply(lambda x: np.mean(x.tolist(), axis=0)).to_dict()\n",
    "\n",
    "# 모든 business_encoded의 리스트\n",
    "all_business_encoded = np.arange(num_businesses)\n",
    "# 기본 sentiment_vector (예: 모든 요소가 0 또는 전체 평균)\n",
    "# Assuming sentiment_vector_dim = 15\n",
    "default_sentiment_vector = np.zeros(15)\n",
    "\n",
    "# 2. train_df에서 각 사용자별로 상호작용한 business_encoded ID 집합 생성\n",
    "user_interacted_businesses = train_df.groupby('user_encoded')['business_encoded'].apply(set).to_dict()\n",
    "\n",
    "# --- Precision@5 계산 함수 ---\n",
    "def calculate_precision_at_k(model, test_df, num_users, all_business_encoded,\n",
    "                             user_interacted_businesses, business_sentiment_map,\n",
    "                             k=5, relevance_threshold=4.0, batch_size=256):\n",
    "    model.eval()\n",
    "    total_precision = 0\n",
    "    num_users_with_recommendations = 0\n",
    "\n",
    "    unique_test_users = test_df['user_encoded'].unique()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user_encoded in unique_test_users:\n",
    "            # 해당 사용자가 훈련 데이터에서 상호작용한 비즈니스 목록\n",
    "            interacted_businesses = user_interacted_businesses.get(user_encoded, set())\n",
    "\n",
    "            # 추천 후보 비즈니스 (훈련 데이터에 없는 모든 비즈니스)\n",
    "            candidate_businesses = [\n",
    "                b_id for b_id in all_business_encoded\n",
    "                if b_id not in interacted_businesses\n",
    "            ]\n",
    "\n",
    "            if not candidate_businesses:\n",
    "                # 이 사용자가 모든 비즈니스와 이미 상호작용했거나 후보가 없는 경우 스킵\n",
    "                continue\n",
    "\n",
    "            # 예측을 위한 데이터 준비\n",
    "            user_ids_tensor = torch.tensor([user_encoded] * len(candidate_businesses), dtype=torch.long)\n",
    "            business_ids_tensor = torch.tensor(candidate_businesses, dtype=torch.long)\n",
    "\n",
    "            # 후보 비즈니스에 대한 sentiment_vector 준비\n",
    "            candidate_sentiment_vectors = []\n",
    "            for b_id in candidate_businesses:\n",
    "                sentiment_vec = business_sentiment_map.get(b_id, default_sentiment_vector)\n",
    "                candidate_sentiment_vectors.append(sentiment_vec)\n",
    "            sentiment_vectors_tensor = torch.tensor(np.array(candidate_sentiment_vectors), dtype=torch.float)\n",
    "\n",
    "            # 배치 단위로 예측 수행 (메모리 효율성)\n",
    "            predictions_list = []\n",
    "            for i in range(0, len(user_ids_tensor), batch_size):\n",
    "                batch_user_ids = user_ids_tensor[i:i + batch_size]\n",
    "                batch_business_ids = business_ids_tensor[i:i + batch_size]\n",
    "                batch_sentiment_vectors = sentiment_vectors_tensor[i:i + batch_size]\n",
    "\n",
    "                batch_preds = model(batch_user_ids, batch_business_ids, batch_sentiment_vectors)\n",
    "                # FIX: batch_preds.tolist() will now correctly return a list because view(-1) ensures it's 1D\n",
    "                predictions_list.extend(batch_preds.tolist())\n",
    "\n",
    "            # 예측 결과를 비즈니스 ID와 연결\n",
    "            predictions_dict = {b_id: pred for b_id, pred in zip(candidate_businesses, predictions_list)}\n",
    "\n",
    "            # 상위 K개 추천 아이템 선정\n",
    "            top_k_recommendations_encoded = sorted(predictions_dict.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "            top_k_recommended_b_ids = [item[0] for item in top_k_recommendations_encoded]\n",
    "\n",
    "            # 실제 관련성 확인 (테스트 세트 내에서만 확인)\n",
    "            # test_df에서 해당 user_encoded가 실제로 높은 평점을 준 business_encoded를 찾습니다.\n",
    "            user_actual_relevant_businesses = set(\n",
    "                test_df[(test_df['user_encoded'] == user_encoded) & (test_df['stars'] >= relevance_threshold)]['business_encoded']\n",
    "            )\n",
    "\n",
    "            hits = 0\n",
    "            for recommended_b_id in top_k_recommended_b_ids:\n",
    "                if recommended_b_id in user_actual_relevant_businesses:\n",
    "                    hits += 1\n",
    "\n",
    "            # Precision 계산\n",
    "            if k > 0:\n",
    "                total_precision += (hits / k)\n",
    "                num_users_with_recommendations += 1\n",
    "\n",
    "    if num_users_with_recommendations == 0:\n",
    "        return 0.0 # 추천할 사용자나 유효한 예측이 없는 경우\n",
    "    return total_precision / num_users_with_recommendations\n",
    "\n",
    "# --- Precision@5 계산 및 출력 ---\n",
    "print(\"\\nPrecision@5 계산 시작...\")\n",
    "precision_at_5 = calculate_precision_at_k(\n",
    "    model, test_df, num_users, all_business_encoded,\n",
    "    user_interacted_businesses, business_sentiment_map,\n",
    "    k=5, relevance_threshold=4.0, batch_size=batch_size\n",
    ")\n",
    "print(f\"Precision@5 (relevance_threshold=4.0): {precision_at_5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecafb108-0a76-4961-9507-a8ae3e075286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
