{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c427a792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82104\\anaconda3\\envs\\absa-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 모델 설정\n",
    "model_name = \"gauneg/deberta-v3-base-absa-ate-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name).eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 레이블 맵 정의 (모델에 따라 상이할 수 있음)\n",
    "labels = {\n",
    "    \"B-neu\": 1,\n",
    "    \"I-neu\": 2,\n",
    "    \"O\": 0,\n",
    "    \"B-neg\": 3,\n",
    "    \"B-con\": 4,\n",
    "    \"I-pos\": 5,\n",
    "    \"B-pos\": 6,\n",
    "    \"I-con\": 7,\n",
    "    \"I-neg\": 8,\n",
    "    \"X\": -100,\n",
    "}\n",
    "id2label = {v: k for k, v in labels.items()}\n",
    "\n",
    "\n",
    "# === ABSA 추출 함수 ===\n",
    "def extract_aspects(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(\n",
    "        device\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        predictions = torch.argmax(probs, dim=-1).squeeze().tolist()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
    "    labels_pred = [id2label.get(p, \"O\") for p in predictions]\n",
    "\n",
    "    aspects, current, sentiment, scores = [], [], None, []\n",
    "    for i, (token, label) in enumerate(zip(tokens, labels_pred)):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current:\n",
    "                avg_score = sum(scores) / len(scores) if scores else 0.0\n",
    "                aspects.append((\"\".join(current), sentiment, round(avg_score, 4)))\n",
    "            current = [token.replace(\"▁\", \" \") if token.startswith(\"▁\") else token]\n",
    "            sentiment = label.split(\"-\")[1]\n",
    "            scores = [probs[0, i, labels[f\"B-{sentiment}\"]].item()]\n",
    "        elif label.startswith(\"I-\") and current:\n",
    "            current.append(token.replace(\"▁\", \" \") if token.startswith(\"▁\") else token)\n",
    "            scores.append(probs[0, i, labels.get(f\"I-{sentiment}\", 0)].item())\n",
    "        else:\n",
    "            if current:\n",
    "                avg_score = sum(scores) / len(scores) if scores else 0.0\n",
    "                aspects.append((\"\".join(current), sentiment, round(avg_score, 4)))\n",
    "                current, sentiment, scores = [], None, []\n",
    "\n",
    "    if current:\n",
    "        avg_score = sum(scores) / len(scores) if scores else 0.0\n",
    "        aspects.append((\"\".join(current), sentiment, round(avg_score, 4)))\n",
    "\n",
    "    clean_aspects = []\n",
    "    for term, sent, score in aspects:\n",
    "        term = term.strip()\n",
    "        if term:\n",
    "            clean_aspects.append(\n",
    "                {\"term\": term, \"sentiment\": sent, \"confidence\": score}  # 감성 점수\n",
    "            )\n",
    "    return clean_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24625073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ABSA 추출 중: 45546it [11:43, 22.98it/s]    "
     ]
    }
   ],
   "source": [
    "input_file = \"review_5up.json\"  # 줄 단위 JSON (user_id, business_id, text 포함)\n",
    "output_file = \"absa_ate_results2.jsonl\"  # 결과 저장 파일\n",
    "\n",
    "# 중복 처리 방지용 (이미 처리된 텍스트)\n",
    "processed = set()\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "                processed.add(item[\"text\"])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f_in, open(\n",
    "    output_file, \"a\", encoding=\"utf-8\"\n",
    ") as f_out:\n",
    "    for line in tqdm(f_in, desc=\"ABSA 추출 중\"):\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            if item[\"text\"] in processed:\n",
    "                continue\n",
    "            aspects = extract_aspects(item[\"text\"])\n",
    "            result = {\n",
    "                \"review_id\": item.get(\"review_id\"),\n",
    "                \"user_id\": item.get(\"user_id\"),\n",
    "                \"business_id\": item.get(\"business_id\"),\n",
    "                \"text\": item[\"text\"],\n",
    "                \"aspects\": aspects,\n",
    "            }\n",
    "            f_out.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "        except:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
