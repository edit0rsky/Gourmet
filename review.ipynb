{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8327b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"data/yelp_academic_dataset_review.json\"\n",
    "\n",
    "# chunk 단위로 전체 데이터 읽고 합치기\n",
    "chunk_size = 100000\n",
    "chunks = pd.read_json(file_path, lines=True, chunksize=chunk_size)\n",
    "\n",
    "# 모든 chunk를 리스트에 담아서 연결\n",
    "df_r = pd.concat(chunk for chunk in chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e925b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84461ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6990280 entries, 0 to 6990279\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   review_id    object        \n",
      " 1   user_id      object        \n",
      " 2   business_id  object        \n",
      " 3   stars        int64         \n",
      " 4   useful       int64         \n",
      " 5   funny        int64         \n",
      " 6   cool         int64         \n",
      " 7   text         object        \n",
      " 8   date         datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(4), object(4)\n",
      "memory usage: 480.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4092fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_review(text):\n",
    "    text = text.strip().lower()                    # 소문자화 + 양쪽 공백 제거\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)            # URL 제거\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?']\", \" \", text)  # 알파벳/숫자/기초 문장부호 외 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text)               # 공백 정리\n",
    "    return text\n",
    "\n",
    "# funny와 cool 컬럼 제거\n",
    "df = df.drop(columns=[\"funny\", \"cool\"])\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"text\"].astype(str).apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca032a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 토큰 길이 계산: 100%|██████████| 6990280/6990280 [19:06<00:00, 6096.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토큰 필터링 완료: 2316286개 리뷰 제거됨 (총 6990280 → 4673994)\n",
      "⏱ 소요 시간: 19분 37초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "# 1. 토크나이저 로드\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "# 2. 토큰 길이 계산 (진행률 + 시간 측정)\n",
    "start = time.time()\n",
    "\n",
    "df[\"token_length\"] = [\n",
    "    len(tokenizer.tokenize(text)) for text in tqdm(df[\"cleaned_text\"], desc=\"🔍 토큰 길이 계산\", total=len(df))\n",
    "]\n",
    "\n",
    "# 3. 필터링: 토큰 길이 1~128 사이만 유지\n",
    "before = len(df)\n",
    "df = df[(df[\"token_length\"] > 0) & (df[\"token_length\"] <= 128)]\n",
    "after = len(df)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "\n",
    "print(f\"✅ 토큰 필터링 완료: {before - after}개 리뷰 제거됨 (총 {before} → {after})\")\n",
    "print(f\"⏱ 소요 시간: {minutes}분 {seconds}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626e7d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 전체 컬럼 포함 JSONL 저장 완료: data/review_0509.json\n",
      "✅ 저장된 리뷰 수: 4673994\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 저장 경로\n",
    "output_path = \"data/review_0509.json\"\n",
    "\n",
    "# 1. datetime → ISO 문자열로 변환\n",
    "df[\"date\"] = df[\"date\"].astype(str)  # 또는 df[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df.to_dict(orient=\"records\"):\n",
    "        json.dump(row, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"📁 전체 컬럼 포함 JSONL 저장 완료: {output_path}\")\n",
    "print(f\"✅ 저장된 리뷰 수: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc822c6",
   "metadata": {},
   "source": [
    "# 근거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a00fd4",
   "metadata": {},
   "source": [
    "① 데이터 Chunk 단위 처리 근거\n",
    "대규모 데이터셋을 처리할 때 메모리 효율성을 높이기 위한 Chunk 단위 처리 방법입니다.\n",
    "\n",
    "참고문헌:\n",
    "McKinney, W. (2011). pandas: a foundational Python library for data analysis and statistics. Python for High Performance and Scientific Computing, 14(9), 1-9.\n",
    "\n",
    "🔗 출처\n",
    "\n",
    "이 문헌은 pandas 라이브러리를 사용한 효율적 데이터 처리 기법으로 chunk 기반 데이터 로딩의 중요성을 설명합니다.\n",
    "\n",
    "② 텍스트 데이터 전처리 근거\n",
    "머신러닝 모델(특히 NLP 모델)을 훈련하기 전 텍스트 데이터를 정제하는 과정의 중요성을 설명하는 문헌입니다.\n",
    "\n",
    "참고문헌:\n",
    "Haddi, E., Liu, X., & Shi, Y. (2013). The role of text pre-processing in sentiment analysis. Procedia Computer Science, 17, 26-32.\n",
    "\n",
    "🔗 출처 https://www.sciencedirect.com/science/article/pii/S1877050913001385\n",
    "\n",
    "이 논문은 텍스트 전처리 과정이 감성 분석 모델의 정확도와 성능에 매우 큰 영향을 미친다고 강조합니다.\n",
    "\n",
    "③ 토큰화 및 길이 기반 필터링 근거\n",
    "모델 훈련 시 최대 길이를 기준으로 입력 데이터를 필터링하여 효율성을 높이는 방법입니다.\n",
    "\n",
    "참고문헌:\n",
    "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of NAACL 2019.\n",
    "\n",
    "🔗 출처 https://aclanthology.org/N19-1423/\n",
    "\n",
    "이 논문은 Transformer 기반 모델에서 입력 시퀀스의 최대 길이를 설정하고 초과하는 데이터를 자르거나 제거하는 것이 효율적임을 언급하며, 성능과 훈련 속도 개선에 영향을 준다고 보고합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fd58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
