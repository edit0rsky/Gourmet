{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Data Preparation Pipeline\n",
    "This notebook consolidates all preprocessing steps used to create `review_business_5up_5aspect_3sentiment_vectorized_clean.json`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 1,
=======
   "execution_count": 13,
>>>>>>> Stashed changes
=======
   "execution_count": 13,
>>>>>>> Stashed changes
=======
   "execution_count": 13,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======
    "import os\n",
>>>>>>> Stashed changes
=======
    "import os\n",
>>>>>>> Stashed changes
=======
    "import os\n",
>>>>>>> Stashed changes
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import DebertaV2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
      "총 6855개 항목이 'data/output/business.json'에 저장되었습니다.\n"
=======
      "총 6855개 항목이 '../data/output/business.json'에 저장되었습니다.\n"
>>>>>>> Stashed changes
=======
      "총 6855개 항목이 '../data/output/business.json'에 저장되었습니다.\n"
>>>>>>> Stashed changes
=======
      "총 6855개 항목이 '../data/output/business.json'에 저장되었습니다.\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "# --- Step 1: business.json preprocessing ---\n",
    "df_B = pd.read_json(\"data/raw/yelp_academic_dataset_business.json\", lines=True)\n",
    "business_df = df_B.copy()\n",
    "drop_cols = ['postal_code','latitude','longitude','attributes','hours']\n",
    "business_df = business_df.drop(columns=drop_cols)\n",
    "business_df.loc[business_df['city'].str.lower().str.contains(\"philadelphia\", na=False),'city'] = \"Philadelphia\"\n",
    "\n",
    "def load_categories(fp):\n",
    "    with open(fp,'r',encoding='utf-8') as f:\n",
    "        return set(line.strip().lower() for line in f if line.strip())\n",
    "food_categories = load_categories('data/raw/food.txt')\n",
    "restaurant_categories = load_categories('data/raw/restaurant.txt')\n",
    "target_categories = food_categories.union(restaurant_categories)\n",
    "\n",
    "def category_match(row):\n",
    "    if isinstance(row,str):\n",
    "        biz_categories = set(cat.strip().lower() for cat in row.split(','))\n",
    "        return bool(biz_categories & target_categories)\n",
    "    return False\n",
    "business_food_df = business_df[business_df['categories'].apply(category_match)]\n",
    "top_state = business_food_df['state'].value_counts().idxmax()\n",
    "business_pa_df = business_food_df[business_food_df['state']==top_state]\n",
    "business_paph_df = business_pa_df[business_pa_df['city']==\"Philadelphia\"]\n",
    "mask = business_paph_df.apply(lambda col: col.map(lambda x: pd.isna(x) or (isinstance(x,str) and x.strip()==\"\"))).any(axis=1)\n",
    "business_paph_df_2 = business_paph_df[~mask].reset_index(drop=True)\n",
    "business_paph_df_2.to_json(\"data/output/business.json\", orient=\"records\", lines=True, force_ascii=False)\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "print(f\"총 {len(business_paph_df_2)}개 항목이 'data/output/business.json'에 저장되었습니다.\")"
=======
    "print(f\"총 {len(business_paph_df_2)}개 항목이 '../data/output/business.json'에 저장되었습니다.\")"
>>>>>>> Stashed changes
=======
    "print(f\"총 {len(business_paph_df_2)}개 항목이 '../data/output/business.json'에 저장되었습니다.\")"
>>>>>>> Stashed changes
=======
    "print(f\"총 {len(business_paph_df_2)}개 항목이 '../data/output/business.json'에 저장되었습니다.\")"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
      "100%|██████████| 730552/730552 [02:11<00:00, 5541.64it/s]\n"
=======
      "100%|██████████| 730552/730552 [02:18<00:00, 5281.29it/s]\n"
>>>>>>> Stashed changes
=======
      "100%|██████████| 730552/730552 [02:18<00:00, 5281.29it/s]\n"
>>>>>>> Stashed changes
=======
      "100%|██████████| 730552/730552 [02:18<00:00, 5281.29it/s]\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "# --- Step 1: review.json preprocessing ---\n",
    "chunk_size=100000\n",
    "chunks=pd.read_json(\"data/raw/yelp_academic_dataset_review.json\", lines=True, chunksize=chunk_size)\n",
    "df_review=pd.concat(chunk for chunk in chunks)\n",
    "business_ids=set(business_paph_df_2['business_id'])\n",
    "df_review=df_review[df_review['business_id'].isin(business_ids)]\n",
    "df_review=df_review.drop(columns=['funny','cool'])\n",
    "\n",
    "tqdm.pandas()\n",
    "tokenizer=DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "df_review['token_length']=df_review['text'].progress_apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df_review.to_json(\"data/output/review.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 6,
>>>>>>> Stashed changes
=======
   "execution_count": 6,
>>>>>>> Stashed changes
=======
   "execution_count": 6,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: user.json preprocessing ---\n",
    "chunks=pd.read_json(\"data/raw/yelp_academic_dataset_user.json\", lines=True, chunksize=100000)\n",
    "df_user=pd.concat(chunk for chunk in chunks)\n",
    "drop_columns=['yelping_since','funny','cool','elite','friends','fans','compliment_hot','compliment_more','compliment_profile','compliment_cute','compliment_list','compliment_note','compliment_plain','compliment_cool','compliment_funny','compliment_writer','compliment_photos']\n",
    "df_user=df_user.drop(columns=drop_columns)\n",
    "review_counts=df_review['user_id'].value_counts()\n",
    "user_ids_5plus=review_counts[review_counts>=5].index\n",
    "df_user=df_user[df_user['user_id'].isin(user_ids_5plus)]\n",
    "df_user.to_json(\"data/output/user.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: merge review, user and business ---\n",
    "def load_jsonl(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "reviews=load_jsonl(\"data/output/review.json\")\n",
    "users=load_jsonl(\"data/output/user.json\")\n",
    "businesses=load_jsonl(\"data/output/business.json\")\n",
    "user_dict={u['user_id']:u for u in users}\n",
    "business_dict={b['business_id']:b for b in businesses}\n",
    "merged_data=[]\n",
    "for r in tqdm(reviews, desc='병합 중'):\n",
    "    uid=r['user_id']; bid=r['business_id']\n",
    "    if uid in user_dict and bid in business_dict:\n",
    "        m=r.copy()\n",
    "        for k,v in user_dict[uid].items():\n",
    "            m[f'user_{k}']=v\n",
    "        for k,v in business_dict[bid].items():\n",
    "            m[f'business_{k}']=v\n",
    "        merged_data.append(m)\n",
    "with open(\"merged_dataset.json\",\"w\",encoding='utf-8') as f:\n",
    "    for row in merged_data:\n",
    "        json.dump(row,f,ensure_ascii=False); f.write(\"\")\n",
    "print(\"병합 완료: merged_dataset.json\")"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "병합 중: 100%|██████████| 730552/730552 [00:03<00:00, 240534.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "병합 완료: data/output/merged_dataset.json\n",
      "총 병합 리뷰 수: 451185\n",
      "샘플 컬럼: ['review_id', 'user_id', 'business_id', 'review_stars', 'review_useful', 'text', 'review_date', 'token_length', 'user_name', 'user_review_count']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: merge review, user and business ---\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# 파일 로딩\n",
    "reviews = load_jsonl(\"data/output/review.json\")\n",
    "users = load_jsonl(\"data/output/user.json\")\n",
    "businesses = load_jsonl(\"data/output/business.json\")\n",
    "\n",
    "user_dict = {u['user_id']: u for u in users}\n",
    "business_dict = {b['business_id']: b for b in businesses}\n",
    "\n",
    "# 병합\n",
    "merged_data = []\n",
    "for r in tqdm(reviews, desc='병합 중'):\n",
    "    uid = r['user_id']\n",
    "    bid = r['business_id']\n",
    "    if uid in user_dict and bid in business_dict:\n",
    "        m = r.copy()\n",
    "        for k, v in user_dict[uid].items():\n",
    "            m[f'user_{k}'] = v\n",
    "        for k, v in business_dict[bid].items():\n",
    "            m[f'business_{k}'] = v\n",
    "        merged_data.append(m)\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df = pd.DataFrame(merged_data)\n",
    "\n",
    "# 불필요한 ID 컬럼 제거\n",
    "df.drop(columns=[\"user_user_id\", \"business_business_id\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 컬럼 이름 리네이밍\n",
    "df.rename(columns={\n",
    "    \"stars\": \"review_stars\",\n",
    "    \"useful\": \"review_useful\",\n",
    "    \"date\": \"review_date\"\n",
    "}, inplace=True)\n",
    "\n",
    "# 저장 경로\n",
    "output_path = \"data/output/merged_dataset.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# JSONL 형식으로 저장\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in df.to_dict(orient=\"records\"):\n",
    "        json.dump(row, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"병합 완료:\", output_path)\n",
    "print(f\"총 병합 리뷰 수: {len(df)}\")\n",
    "print(\"샘플 컬럼:\", df.columns[:10].tolist())\n"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: attach IDs to ABSA results and vectorize ---\n",
    "# filter merged dataset for users with >=5 reviews\n",
    "user_review_counts=defaultdict(int)\n",
    "with open(\"merged_dataset.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        user_review_counts[obj['user_id']]+=1\n",
    "qualified_users={u for u,c in user_review_counts.items() if c>=5}\n",
    "filtered_reviews=[]\n",
    "with open(\"merged_dataset.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        if obj['user_id'] in qualified_users:\n",
    "            filtered_reviews.append(obj)\n",
    "with open(\"merged_dataset_5up_users_only.json\",\"w\",encoding='utf-8') as f:\n",
    "    for obj in filtered_reviews:\n",
    "        f.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"✅ 필터링 완료:\",len(filtered_reviews),'개 리뷰 저장 → merged_dataset_5up_users_only.json')\n",
    "\n",
    "id_map={}\n",
    "for obj in filtered_reviews:\n",
    "    rid=obj['review_id']\n",
    "    id_map[rid]={\n",
    "        'user_id':obj['user_id'],\n",
    "        'business_id':obj['business_id'],\n",
    "        'stars':obj['review_stars'],\n",
    "        'review_useful':obj['review_useful'],\n",
    "        'review_date':obj['review_date']\n",
    "    }\n",
    "updated=[]\n",
    "with open(\"review_5up_5aspect_3sentiment.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc='ID 및 평점 추가 중'):\n",
    "        obj=json.loads(line)\n",
    "        rid=obj.get('review_id')\n",
    "        if rid in id_map:\n",
    "            obj.update(id_map[rid])\n",
    "            updated.append(obj)\n",
    "with open(\"review_5up_5aspect_3sentiment_with_ids.json\",\"w\",encoding='utf-8') as f:\n",
    "    for obj in updated:\n",
    "        f.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"저장 완료:\",len(updated),'건 → review_5up_5aspect_3sentiment_with_ids.json')\n",
    "\n",
    "input_file=\"review_5up_5aspect_3sentiment_with_ids.json\"\n",
    "output_file=\"review_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "\n",
    "def sentiment_to_vector(sentiment_dict):\n",
    "    aspects=['food','service','price','ambience','location']\n",
    "    polarities=['Negative','Neutral','Positive']\n",
    "    vector=[]\n",
    "    for asp in aspects:\n",
    "        scores=sentiment_dict.get(asp,{}).get('scores',{})\n",
    "        for pol in polarities:\n",
    "            vector.append(scores.get(pol,0.0))\n",
    "    return vector\n",
    "with open(input_file,'r',encoding='utf-8') as fin, open(output_file,'w',encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        obj=json.loads(line)\n",
    "        vec=sentiment_to_vector(obj.get('sentiment',{}))\n",
    "        cleaned={'review_id':obj.get('review_id'),'user_id':obj.get('user_id'),'business_id':obj.get('business_id'),'stars':obj.get('stars'),'review_date':obj.get('review_date'),'sentiment_vector':vec}\n",
    "        fout.write(json.dumps(cleaned,ensure_ascii=False)+\"\")\n",
    "print(\"완료: text와 sentiment 제거 후 저장 →\",output_file)"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 19,
   "id": "93157bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 장치: mps\n",
      "143개의 리뷰 ID를 건너뜁니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ABSA 감성 분석 진행 중:   0%|          | 223/451185 [00:35<35:20:46,  3.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "ABSA 감성 분석 진행 중:   0%|          | 300/451185 [01:16<90:28:05,  1.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "ABSA 감성 분석 진행 중:   0%|          | 300/451185 [01:16<32:08:30,  3.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 75\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m review[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m processed_ids:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     74\u001b[0m sentiment_result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 75\u001b[0m     asp: analyze_sentiment(review[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], asp)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m asp \u001b[38;5;129;01min\u001b[39;00m aspects\n\u001b[1;32m     77\u001b[0m }\n\u001b[1;32m     79\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: review[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: review[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentiment_result\n\u001b[1;32m     83\u001b[0m }\n\u001b[1;32m     85\u001b[0m f_out\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(result, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 41\u001b[0m, in \u001b[0;36manalyze_sentiment\u001b[0;34m(text, aspect)\u001b[0m\n\u001b[1;32m     38\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     39\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m: {model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label[i]: \u001b[38;5;28mfloat\u001b[39m(probs[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(probs))},\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m     }\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_message\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(e)}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Step 2.5: Run ABSA model (Huggingface 기반 + MPS 지원 + 경고 제거 + ETA) ---\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 장치 설정\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "\n",
    "# 모델 로드 (fast tokenizer 비활성화)\n",
    "model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 분석할 측면 목록\n",
    "aspects = [\"food\", \"service\", \"price\", \"ambience\", \"location\"]\n",
    "\n",
    "# 감정 분석 함수\n",
    "def analyze_sentiment(text, aspect):\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            text, aspect,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)[0]\n",
    "        return {\n",
    "            \"scores\": {model.config.id2label[i]: float(probs[i]) for i in range(len(probs))},\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"scores\": None, \"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "# 경로 설정\n",
    "input_file = \"data/output/merged_dataset.json\"\n",
    "output_file = \"data/output/review_5up_5aspect_3sentiment.json\"\n",
    "\n",
    "# 이미 처리된 리뷰 ID 수집\n",
    "processed_ids = set()\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                processed_ids.add(json.loads(line)[\"review_id\"])\n",
    "            except:\n",
    "                continue\n",
    "    print(f\"{len(processed_ids)}개의 리뷰 ID를 건너뜁니다.\")\n",
    "\n",
    "# 전체 입력 라인 수 계산 (잔여시간 표시용)\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "# 감성 분석 실행\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f_in, open(output_file, \"a\", encoding=\"utf-8\") as f_out:\n",
    "    for line in tqdm(f_in, total=total_lines, desc=\"ABSA 감성 분석 진행 중\"):\n",
    "        try:\n",
    "            review = json.loads(line.strip())\n",
    "            if review[\"review_id\"] in processed_ids:\n",
    "                continue\n",
    "\n",
    "            sentiment_result = {\n",
    "                asp: analyze_sentiment(review[\"text\"], asp)\n",
    "                for asp in aspects\n",
    "            }\n",
    "\n",
    "            result = {\n",
    "                \"review_id\": review[\"review_id\"],\n",
    "                \"text\": review[\"text\"],\n",
    "                \"sentiment\": sentiment_result\n",
    "            }\n",
    "\n",
    "            f_out.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"ABSA 분석 완료. 결과 저장 경로: {output_file}\")"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 완료: 451185 개 리뷰 저장 → merged_dataset_5up_users_only.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/output/review_5up_5aspect_3sentiment.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 31\u001b[0m\n\u001b[1;32m     23\u001b[0m     id_map[rid]\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m:obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m:obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_date\u001b[39m\u001b[38;5;124m'\u001b[39m:obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m     }\n\u001b[1;32m     30\u001b[0m updated\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/output/review_5up_5aspect_3sentiment.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(f, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID 및 평점 추가 중\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     33\u001b[0m         obj\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/output/review_5up_5aspect_3sentiment.json'"
     ]
    }
   ],
   "source": [
    "# --- Step 3: attach IDs to ABSA results and vectorize ---\n",
    "# filter merged dataset for users with >=5 reviews\n",
    "user_review_counts=defaultdict(int)\n",
    "with open(\"data/output/merged_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        for obj in json.loads(f\"[{line.strip().replace('}{', '},{')}]\"):\n",
    "            user_review_counts[obj['user_id']] += 1\n",
    "qualified_users = {u for u, c in user_review_counts.items() if c >= 5}\n",
    "filtered_reviews=[]\n",
    "with open(\"data/output/merged_dataset.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        if obj['user_id'] in qualified_users:\n",
    "            filtered_reviews.append(obj)\n",
    "with open(\"data/output/merged_dataset_5up_users_only.json\",\"w\",encoding='utf-8') as f:\n",
    "    for obj in filtered_reviews:\n",
    "        f.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"필터링 완료:\",len(filtered_reviews),'개 리뷰 저장 → merged_dataset_5up_users_only.json')\n",
    "\n",
    "id_map={}\n",
    "for obj in filtered_reviews:\n",
    "    rid=obj['review_id']\n",
    "    id_map[rid]={\n",
    "        'user_id':obj['user_id'],\n",
    "        'business_id':obj['business_id'],\n",
    "        'stars':obj['review_stars'],\n",
    "        'review_useful':obj['review_useful'],\n",
    "        'review_date':obj['review_date']\n",
    "    }\n",
    "updated=[]\n",
    "with open(\"data/output/review_5up_5aspect_3sentiment.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc='ID 및 평점 추가 중'):\n",
    "        obj=json.loads(line)\n",
    "        rid=obj.get('review_id')\n",
    "        if rid in id_map:\n",
    "            obj.update(id_map[rid])\n",
    "            updated.append(obj)\n",
    "with open(\"data/output/review_5up_5aspect_3sentiment_with_ids.json\",\"w\",encoding='utf-8') as f:\n",
    "    for obj in updated:\n",
    "        f.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"저장 완료:\",len(updated),'건 → review_5up_5aspect_3sentiment_with_ids.json')\n",
    "\n",
    "input_file=\"data/output/review_5up_5aspect_3sentiment_with_ids.json\"\n",
    "output_file=\"data/output/review_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "\n",
    "def sentiment_to_vector(sentiment_dict):\n",
    "    aspects=['food','service','price','ambience','location']\n",
    "    polarities=['Negative','Neutral','Positive']\n",
    "    vector=[]\n",
    "    for asp in aspects:\n",
    "        scores=sentiment_dict.get(asp,{}).get('scores',{})\n",
    "        for pol in polarities:\n",
    "            vector.append(scores.get(pol,0.0))\n",
    "    return vector\n",
    "with open(input_file,'r',encoding='utf-8') as fin, open(output_file,'w',encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        obj=json.loads(line)\n",
    "        vec=sentiment_to_vector(obj.get('sentiment',{}))\n",
    "        cleaned={'review_id':obj.get('review_id'),'user_id':obj.get('user_id'),'business_id':obj.get('business_id'),'stars':obj.get('stars'),'review_date':obj.get('review_date'),'sentiment_vector':vec}\n",
    "        fout.write(json.dumps(cleaned,ensure_ascii=False)+\"\")\n",
    "print(\"완료: text와 sentiment 제거 후 저장 →\",output_file)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: filter users with <5 unique businesses ---\n",
    "input_file=\"review_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "output_file=\"review_business_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "user_biz_ids=defaultdict(set)\n",
    "with open(input_file,'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        user_biz_ids[obj['user_id']].add(obj['business_id'])\n",
    "with open(input_file,'r',encoding='utf-8') as fin, open(output_file,'w',encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        obj=json.loads(line)\n",
    "        if len(user_biz_ids[obj['user_id']])>=5:\n",
    "            fout.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "print(\"완료: business_id가 5개 미만인 사용자 제거 후 저장 →\",output_file)"
=======
    "print(\"✅ 완료: business_id가 5개 미만인 사용자 제거 후 저장 →\",output_file)"
>>>>>>> Stashed changes
=======
    "print(\"✅ 완료: business_id가 5개 미만인 사용자 제거 후 저장 →\",output_file)"
>>>>>>> Stashed changes
=======
    "print(\"✅ 완료: business_id가 5개 미만인 사용자 제거 후 저장 →\",output_file)"
>>>>>>> Stashed changes
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
