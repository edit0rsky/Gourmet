{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Data Preparation Pipeline\n",
    "This notebook consolidates all preprocessing steps used to create `review_business_5up_5aspect_3sentiment_vectorized_clean.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import DebertaV2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 6855개 항목이 'data/output/business.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: business.json preprocessing ---\n",
    "df_B = pd.read_json(\"data/raw/yelp_academic_dataset_business.json\", lines=True)\n",
    "business_df = df_B.copy()\n",
    "drop_cols = ['postal_code','latitude','longitude','attributes','hours']\n",
    "business_df = business_df.drop(columns=drop_cols)\n",
    "business_df.loc[business_df['city'].str.lower().str.contains(\"philadelphia\", na=False),'city'] = \"Philadelphia\"\n",
    "\n",
    "def load_categories(fp):\n",
    "    with open(fp,'r',encoding='utf-8') as f:\n",
    "        return set(line.strip().lower() for line in f if line.strip())\n",
    "food_categories = load_categories('data/raw/food.txt')\n",
    "restaurant_categories = load_categories('data/raw/restaurant.txt')\n",
    "target_categories = food_categories.union(restaurant_categories)\n",
    "\n",
    "def category_match(row):\n",
    "    if isinstance(row,str):\n",
    "        biz_categories = set(cat.strip().lower() for cat in row.split(','))\n",
    "        return bool(biz_categories & target_categories)\n",
    "    return False\n",
    "business_food_df = business_df[business_df['categories'].apply(category_match)]\n",
    "top_state = business_food_df['state'].value_counts().idxmax()\n",
    "business_pa_df = business_food_df[business_food_df['state']==top_state]\n",
    "business_paph_df = business_pa_df[business_pa_df['city']==\"Philadelphia\"]\n",
    "mask = business_paph_df.apply(lambda col: col.map(lambda x: pd.isna(x) or (isinstance(x,str) and x.strip()==\"\"))).any(axis=1)\n",
    "business_paph_df_2 = business_paph_df[~mask].reset_index(drop=True)\n",
    "business_paph_df_2.to_json(\"data/output/business.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "print(f\"총 {len(business_paph_df_2)}개 항목이 'data/output/business.json'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730552/730552 [02:11<00:00, 5541.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: review.json preprocessing ---\n",
    "chunk_size=100000\n",
    "chunks=pd.read_json(\"data/raw/yelp_academic_dataset_review.json\", lines=True, chunksize=chunk_size)\n",
    "df_review=pd.concat(chunk for chunk in chunks)\n",
    "business_ids=set(business_paph_df_2['business_id'])\n",
    "df_review=df_review[df_review['business_id'].isin(business_ids)]\n",
    "df_review=df_review.drop(columns=['funny','cool'])\n",
    "\n",
    "tqdm.pandas()\n",
    "tokenizer=DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "df_review['token_length']=df_review['text'].progress_apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "df_review.to_json(\"data/output/review.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: user.json preprocessing ---\n",
    "chunks=pd.read_json(\"data/raw/yelp_academic_dataset_user.json\", lines=True, chunksize=100000)\n",
    "df_user=pd.concat(chunk for chunk in chunks)\n",
    "drop_columns=['yelping_since','funny','cool','elite','friends','fans','compliment_hot','compliment_more','compliment_profile','compliment_cute','compliment_list','compliment_note','compliment_plain','compliment_cool','compliment_funny','compliment_writer','compliment_photos']\n",
    "df_user=df_user.drop(columns=drop_columns)\n",
    "review_counts=df_review['user_id'].value_counts()\n",
    "user_ids_5plus=review_counts[review_counts>=5].index\n",
    "df_user=df_user[df_user['user_id'].isin(user_ids_5plus)]\n",
    "df_user.to_json(\"data/output/user.json\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: merge review, user and business ---\n",
    "def load_jsonl(path):\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "reviews=load_jsonl(\"data/output/review.json\")\n",
    "users=load_jsonl(\"data/output/user.json\")\n",
    "businesses=load_jsonl(\"data/output/business.json\")\n",
    "user_dict={u['user_id']:u for u in users}\n",
    "business_dict={b['business_id']:b for b in businesses}\n",
    "merged_data=[]\n",
    "for r in tqdm(reviews, desc='병합 중'):\n",
    "    uid=r['user_id']; bid=r['business_id']\n",
    "    if uid in user_dict and bid in business_dict:\n",
    "        m=r.copy()\n",
    "        for k,v in user_dict[uid].items():\n",
    "            m[f'user_{k}']=v\n",
    "        for k,v in business_dict[bid].items():\n",
    "            m[f'business_{k}']=v\n",
    "        merged_data.append(m)\n",
    "with open(\"merged_dataset.json\",\"w\",encoding='utf-8') as f:\n",
    "    for row in merged_data:\n",
    "        json.dump(row,f,ensure_ascii=False); f.write(\"\")\n",
    "print(\"병합 완료: merged_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: attach IDs to ABSA results and vectorize ---\n",
    "# filter merged dataset for users with >=5 reviews\n",
    "user_review_counts=defaultdict(int)\n",
    "with open(\"merged_dataset.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        user_review_counts[obj['user_id']]+=1\n",
    "qualified_users={u for u,c in user_review_counts.items() if c>=5}\n",
    "filtered_reviews=[]\n",
    "with open(\"merged_dataset.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        if obj['user_id'] in qualified_users:\n",
    "            filtered_reviews.append(obj)\n",
    "with open(\"merged_dataset_5up_users_only.json\",\"w\",encoding='utf-8') as f:\n",
    "    for obj in filtered_reviews:\n",
    "        f.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"✅ 필터링 완료:\",len(filtered_reviews),'개 리뷰 저장 → merged_dataset_5up_users_only.json')\n",
    "\n",
    "id_map={}\n",
    "for obj in filtered_reviews:\n",
    "    rid=obj['review_id']\n",
    "    id_map[rid]={\n",
    "        'user_id':obj['user_id'],\n",
    "        'business_id':obj['business_id'],\n",
    "        'stars':obj['review_stars'],\n",
    "        'review_useful':obj['review_useful'],\n",
    "        'review_date':obj['review_date']\n",
    "    }\n",
    "updated=[]\n",
    "with open(\"review_5up_5aspect_3sentiment.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc='ID 및 평점 추가 중'):\n",
    "        obj=json.loads(line)\n",
    "        rid=obj.get('review_id')\n",
    "        if rid in id_map:\n",
    "            obj.update(id_map[rid])\n",
    "            updated.append(obj)\n",
    "with open(\"review_5up_5aspect_3sentiment_with_ids.json\",\"w\",encoding='utf-8') as f:\n",
    "    for obj in updated:\n",
    "        f.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"저장 완료:\",len(updated),'건 → review_5up_5aspect_3sentiment_with_ids.json')\n",
    "\n",
    "input_file=\"review_5up_5aspect_3sentiment_with_ids.json\"\n",
    "output_file=\"review_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "\n",
    "def sentiment_to_vector(sentiment_dict):\n",
    "    aspects=['food','service','price','ambience','location']\n",
    "    polarities=['Negative','Neutral','Positive']\n",
    "    vector=[]\n",
    "    for asp in aspects:\n",
    "        scores=sentiment_dict.get(asp,{}).get('scores',{})\n",
    "        for pol in polarities:\n",
    "            vector.append(scores.get(pol,0.0))\n",
    "    return vector\n",
    "with open(input_file,'r',encoding='utf-8') as fin, open(output_file,'w',encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        obj=json.loads(line)\n",
    "        vec=sentiment_to_vector(obj.get('sentiment',{}))\n",
    "        cleaned={'review_id':obj.get('review_id'),'user_id':obj.get('user_id'),'business_id':obj.get('business_id'),'stars':obj.get('stars'),'review_date':obj.get('review_date'),'sentiment_vector':vec}\n",
    "        fout.write(json.dumps(cleaned,ensure_ascii=False)+\"\")\n",
    "print(\"완료: text와 sentiment 제거 후 저장 →\",output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: filter users with <5 unique businesses ---\n",
    "input_file=\"review_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "output_file=\"review_business_5up_5aspect_3sentiment_vectorized_clean.json\"\n",
    "user_biz_ids=defaultdict(set)\n",
    "with open(input_file,'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj=json.loads(line)\n",
    "        user_biz_ids[obj['user_id']].add(obj['business_id'])\n",
    "with open(input_file,'r',encoding='utf-8') as fin, open(output_file,'w',encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        obj=json.loads(line)\n",
    "        if len(user_biz_ids[obj['user_id']])>=5:\n",
    "            fout.write(json.dumps(obj,ensure_ascii=False)+\"\")\n",
    "print(\"완료: business_id가 5개 미만인 사용자 제거 후 저장 →\",output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
