{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673869bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 벡터 생성 중: 452505it [2:17:08, 54.99it/s]\n",
      "👤 유저 벡터 평균: 100%|██████████| 28483/28483 [00:13<00:00, 2082.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ user_vector.json 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 설정\n",
    "ABSA_FILE = \"absa_ate_results.json\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "SENT2WEIGHT = {\"pos\": 1.0, \"neg\": -1.0}\n",
    "EMBED_DIM = 384  # 임베딩 차원\n",
    "\n",
    "# 모델 로드\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# 유저별 누적 벡터\n",
    "user_vecs = defaultdict(list)\n",
    "\n",
    "with open(ABSA_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"🔄 벡터 생성 중\"):\n",
    "        review = json.loads(line)\n",
    "        user_id = review[\"user_id\"]\n",
    "        aspects = review.get(\"aspects\", [])\n",
    "\n",
    "        # 🔸 aspects가 없으면 스킵\n",
    "        if not aspects:\n",
    "            continue\n",
    "\n",
    "        for asp in aspects:\n",
    "            term = asp[\"term\"].strip()\n",
    "            sentiment = asp[\"sentiment\"]\n",
    "            confidence = asp[\"confidence\"]\n",
    "\n",
    "            if sentiment not in SENT2WEIGHT:\n",
    "                continue\n",
    "\n",
    "            weight = SENT2WEIGHT[sentiment] * confidence\n",
    "            vec = model.encode(term)\n",
    "            user_vecs[user_id].append(weight * vec)\n",
    "\n",
    "# 평균 벡터 계산\n",
    "user_embed = {\n",
    "    uid: (np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)).tolist()\n",
    "    for uid, vecs in tqdm(user_vecs.items(), desc=\"👤 유저 벡터 평균\")\n",
    "}\n",
    "\n",
    "# 저장\n",
    "with open(\"user_vector.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(user_embed, f, indent=2)\n",
    "\n",
    "print(\"✅ user_vector.json 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d756ff0",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc6b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\82104\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Word2Vec 로딩 중...\n",
      "✅ Word2Vec 로딩 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "👤 유저 벡터 생성 중: 452505it [00:07, 57646.92it/s]\n",
      "👤 유저 벡터 평균: 100%|██████████| 8933/8933 [00:10<00:00, 879.80it/s] \n",
      "🏠 식당 벡터 생성 중: 452505it [00:06, 66231.52it/s]\n",
      "🏠 식당 벡터 평균: 100%|██████████| 3325/3325 [00:00<00:00, 34719.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# 설정\n",
    "ABSA_FILE = \"absa_ate_results.json\"\n",
    "W2V_PATH = \"GoogleNews-vectors-negative300.bin.gz\"  # Word2Vec 모델\n",
    "EMBED_DIM = 300\n",
    "SENT2WEIGHT = {\"pos\": 1.0, \"neg\": -1.0}\n",
    "MIN_LEN = 2        # term 최소 단어 수\n",
    "MIN_CONF = 0.7     # confidence threshold\n",
    "\n",
    "def is_valid_term(term, confidence, stop_words, min_len=2, min_conf=0.7):\n",
    "    tokens = term.strip().split()\n",
    "    if len(tokens) < min_len:\n",
    "        return False\n",
    "    if confidence < min_conf:\n",
    "        return False\n",
    "    if all(tok.lower() in stop_words for tok in tokens):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "print(\"🔁 Word2Vec 로딩 중...\")\n",
    "w2v_model = KeyedVectors.load_word2vec_format(W2V_PATH, binary=True)\n",
    "print(\"✅ Word2Vec 로딩 완료!\")\n",
    "\n",
    "user_vecs = defaultdict(list)\n",
    "\n",
    "with open(ABSA_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"👤 유저 벡터 생성 중\"):\n",
    "        obj = json.loads(line)\n",
    "        uid = obj[\"user_id\"]\n",
    "        for asp in obj.get(\"aspects\", []):\n",
    "            term = asp[\"term\"].strip()\n",
    "            sent = asp[\"sentiment\"]\n",
    "            conf = asp[\"confidence\"]\n",
    "\n",
    "            if sent not in SENT2WEIGHT:\n",
    "                continue\n",
    "            if not is_valid_term(term, conf, stop_words, MIN_LEN, MIN_CONF):\n",
    "                continue\n",
    "\n",
    "            tokens = term.split()\n",
    "            vecs = [w2v_model[tok] for tok in tokens if tok in w2v_model]\n",
    "            if not vecs:\n",
    "                continue\n",
    "\n",
    "            vec = np.mean(vecs, axis=0)\n",
    "            weight = SENT2WEIGHT[sent] * conf\n",
    "            user_vecs[uid].append(weight * vec)\n",
    "\n",
    "user_embed = {\n",
    "    uid: (np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)).tolist()\n",
    "    for uid, vecs in tqdm(user_vecs.items(), desc=\"👤 유저 벡터 평균\")\n",
    "}\n",
    "\n",
    "with open(\"user_vector_filtered.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(user_embed, f, indent=2)\n",
    "\n",
    "biz_vecs = defaultdict(list)\n",
    "\n",
    "with open(ABSA_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"🏠 식당 벡터 생성 중\"):\n",
    "        obj = json.loads(line)\n",
    "        bid = obj[\"business_id\"]\n",
    "        for asp in obj.get(\"aspects\", []):\n",
    "            term = asp[\"term\"].strip()\n",
    "            sent = asp[\"sentiment\"]\n",
    "            conf = asp[\"confidence\"]\n",
    "\n",
    "            if sent not in SENT2WEIGHT:\n",
    "                continue\n",
    "            if not is_valid_term(term, conf, stop_words, MIN_LEN, MIN_CONF):\n",
    "                continue\n",
    "\n",
    "            tokens = term.split()\n",
    "            vecs = [w2v_model[tok] for tok in tokens if tok in w2v_model]\n",
    "            if not vecs:\n",
    "                continue\n",
    "\n",
    "            vec = np.mean(vecs, axis=0)\n",
    "            weight = SENT2WEIGHT[sent] * conf\n",
    "            biz_vecs[bid].append(weight * vec)\n",
    "\n",
    "biz_embed = {\n",
    "    bid: (np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)).tolist()\n",
    "    for bid, vecs in tqdm(biz_vecs.items(), desc=\"🏠 식당 벡터 평균\")\n",
    "}\n",
    "\n",
    "with open(\"business_vector_filtered.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(biz_embed, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
