{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7372fc-9fc8-4a80-b412-36de6751e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 학습,검증,테스트 데이터 7:1:2로 분할\n",
    "2. MLP로 사용자-식당 간의 비선형적 패턴 학습(64차원)\n",
    "3. ABSA결과를 ReviewAspectModule에 입력해 MLP로 학습(32차원)\n",
    "4. 2번과 3번을 결합(concatenation)해 96차원 벡터 만듬\n",
    "5. 4번을 prediction_mlp에 넣어 별점 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade99f3-0a29-4b9d-97b3-b74b82ccf3b6",
   "metadata": {},
   "source": [
    "##### import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os # 모델 저장을 위해 os 모듈 임포트\n",
    "\n",
    "# JSONL 파일 로드 (lines=True 필수)\n",
    "# 파일 경로를 실제 파일 경로로 바꿔주세요.\n",
    "df = pd.read_json('review_business_5up_5aspect_3sentiment_vectorized_clean.json', lines=True)\n",
    "\n",
    "# 필요한 컬럼 추출 (여기서 명시적으로 .copy()를 사용하여 새로운 DataFrame을 만듭니다)\n",
    "df_processed = df[['user_id', 'business_id', 'stars', 'sentiment_vector']].copy()\n",
    "\n",
    "# user_id와 business_id를 연속적인 정수 ID로 인코딩\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "# .loc를 사용하여 값을 할당하여 경고를 방지합니다.\n",
    "df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_businesses = len(business_encoder.classes_)\n",
    "\n",
    "# 1. 먼저 학습+검증 세트와 테스트 세트로 8:2 비율로 분할\n",
    "# (7+1):2 = 8:2\n",
    "train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. 학습+검증 세트를 다시 학습 세트와 검증 세트로 7:1 비율로 분할\n",
    "# val_size_ratio는 train_val_df의 1/8이므로, 전체 데이터의 1/10이 됩니다.\n",
    "val_size_ratio = 1 / 8\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size_ratio, random_state=42)\n",
    "\n",
    "print(f\"전체 데이터 수: {len(df_processed)}\")\n",
    "print(f\"학습 데이터 수: {len(train_df)} ({len(train_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"검증 데이터 수: {len(val_df)} ({len(val_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"테스트 데이터 수: {len(test_df)} ({len(test_df)/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "# --- PyTorch Dataset 및 DataLoader 정의 ---\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.user_ids = torch.tensor(df['user_encoded'].values, dtype=torch.long)\n",
    "        self.business_ids = torch.tensor(df['business_encoded'].values, dtype=torch.long)\n",
    "        self.sentiment_vectors = torch.tensor(np.array(df['sentiment_vector'].tolist()), dtype=torch.float)\n",
    "        self.stars = torch.tensor(df['stars'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.business_ids[idx], self.sentiment_vectors[idx], self.stars[idx]\n",
    "\n",
    "# --- 모델 아키텍처 정의 (이전과 동일) ---\n",
    "class CustomerRestaurantInteractionModule(nn.Module):\n",
    "    def __init__(self, num_users, num_businesses, embedding_dim, mlp_dims):\n",
    "        super(CustomerRestaurantInteractionModule, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.business_embedding = nn.Embedding(num_businesses, embedding_dim)\n",
    "        layers = []\n",
    "        input_dim = embedding_dim * 2\n",
    "        for dim in mlp_dims:\n",
    "            layers.append(nn.Linear(input_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = dim\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output_dim = mlp_dims[-1] if mlp_dims else embedding_dim * 2\n",
    "\n",
    "    def forward(self, user_ids, business_ids):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        business_vec = self.business_embedding(business_ids)\n",
    "        combined_vec = torch.cat((user_vec, business_vec), dim=1)\n",
    "        interaction_features = self.mlp(combined_vec)\n",
    "        return interaction_features\n",
    "\n",
    "class ReviewAspectModule(nn.Module):\n",
    "    def __init__(self, sentiment_vector_dim, mlp_dims):\n",
    "        super(ReviewAspectModule, self).__init__()\n",
    "        layers = []\n",
    "        input_dim = sentiment_vector_dim\n",
    "        for dim in mlp_dims:\n",
    "            layers.append(nn.Linear(input_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = dim\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output_dim = mlp_dims[-1] if mlp_dims else sentiment_vector_dim\n",
    "\n",
    "    def forward(self, sentiment_vectors):\n",
    "        aspect_features = self.mlp(sentiment_vectors)\n",
    "        return aspect_features\n",
    "\n",
    "class AATRec(nn.Module):\n",
    "    def __init__(self, num_users, num_businesses, embedding_dim,\n",
    "                 user_biz_mlp_dims, aspect_mlp_dims, final_mlp_dims,\n",
    "                 sentiment_vector_dim=15):\n",
    "        super(AATRec, self).__init__()\n",
    "        self.customer_restaurant_interaction_module = CustomerRestaurantInteractionModule(\n",
    "            num_users, num_businesses, embedding_dim, user_biz_mlp_dims\n",
    "        )\n",
    "        self.review_aspect_module = ReviewAspectModule(\n",
    "            sentiment_vector_dim, aspect_mlp_dims\n",
    "        )\n",
    "        final_input_dim = self.customer_restaurant_interaction_module.output_dim + \\\n",
    "                          self.review_aspect_module.output_dim\n",
    "        layers = []\n",
    "        input_dim = final_input_dim\n",
    "        for dim in final_mlp_dims:\n",
    "            layers.append(nn.Linear(input_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.prediction_mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user_ids, business_ids, sentiment_vectors):\n",
    "        user_biz_features = self.customer_restaurant_interaction_module(user_ids, business_ids)\n",
    "        aspect_features = self.review_aspect_module(sentiment_vectors)\n",
    "        combined_features = torch.cat((user_biz_features, aspect_features), dim=1)\n",
    "        predicted_rating = self.prediction_mlp(combined_features)\n",
    "        return predicted_rating.squeeze()\n",
    "\n",
    "# --- 하이퍼파라미터 설정 ---\n",
    "embedding_dim = 64\n",
    "user_biz_mlp_dims = [128, 64]\n",
    "aspect_mlp_dims = [64, 32]\n",
    "final_mlp_dims = [64, 32]\n",
    "learning_rate = 0.001\n",
    "epochs = 50 # 조기 종료를 고려하여 충분히 큰 에폭 설정\n",
    "batch_size = 256\n",
    "\n",
    "# --- 조기 종료(Early Stopping) 설정 ---\n",
    "patience = 5 # 검증 RMSE가 개선되지 않아도 기다릴 에폭 수\n",
    "min_delta = 0.001 # 검증 RMSE가 개선되었다고 판단할 최소 변화량\n",
    "best_val_rmse = float('inf') # 초기 최적 검증 RMSE (무한대로 설정)\n",
    "epochs_no_improve = 0 # 검증 RMSE가 개선되지 않은 에폭 수 카운터\n",
    "model_path = 'best_aatrec_model.pt' # 최적 모델 저장 경로\n",
    "\n",
    "# 데이터셋 및 DataLoader 생성\n",
    "train_dataset = ReviewDataset(train_df)\n",
    "val_dataset = ReviewDataset(val_df)\n",
    "test_dataset = ReviewDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = AATRec(num_users, num_businesses, embedding_dim,\n",
    "               user_biz_mlp_dims, aspect_mlp_dims, final_mlp_dims,\n",
    "               sentiment_vector_dim=15)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- 학습 루프 (조기 종료 추가) ---\n",
    "print(\"모델 학습 시작...\")\n",
    "for epoch in range(epochs):\n",
    "    # --- 학습 단계 ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for user_ids, business_ids, sentiment_vectors, stars in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(user_ids, business_ids, sentiment_vectors)\n",
    "        loss = criterion(predictions, stars)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # --- 검증 단계 ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_true_ratings = []\n",
    "    with torch.no_grad():\n",
    "        for user_ids, business_ids, sentiment_vectors, stars in val_loader:\n",
    "            predictions = model(user_ids, business_ids, sentiment_vectors)\n",
    "            loss = criterion(predictions, stars)\n",
    "            total_val_loss += loss.item()\n",
    "            val_predictions.extend(predictions.tolist())\n",
    "            val_true_ratings.extend(stars.tolist())\n",
    "\n",
    "    current_val_rmse = np.sqrt(mean_squared_error(val_true_ratings, val_predictions))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "          f\"Train Loss: {total_train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Val Loss: {total_val_loss / len(val_loader):.4f}, \"\n",
    "          f\"Val RMSE: {current_val_rmse:.4f}\")\n",
    "\n",
    "    # --- 조기 종료 로직 ---\n",
    "    if current_val_rmse < best_val_rmse - min_delta: # 검증 RMSE가 개선되었다면\n",
    "        best_val_rmse = current_val_rmse\n",
    "        epochs_no_improve = 0 # 개선이 있었으므로 카운트 초기화\n",
    "        torch.save(model.state_dict(), model_path) # 최적 모델 가중치 저장\n",
    "        print(f\"  --> 검증 RMSE 개선됨. 최적 모델 저장됨: {best_val_rmse:.4f}\")\n",
    "    else: # 개선되지 않았다면\n",
    "        epochs_no_improve += 1 # 카운트 증가\n",
    "        print(f\"  --> 검증 RMSE 개선되지 않음. 대기 중... ({epochs_no_improve}/{patience})\")\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"조기 종료! {patience} 에폭 동안 검증 RMSE 개선이 없었습니다.\")\n",
    "            break # 학습 루프 종료\n",
    "\n",
    "print(\"모델 학습 완료.\")\n",
    "\n",
    "# --- 최종 테스트 및 RMSE 계산 ---\n",
    "# 최적의 모델 가중치를 로드하여 테스트\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(f\"최적의 모델 가중치 '{model_path}' 로드 완료.\")\n",
    "else:\n",
    "    print(f\"최적의 모델 가중치 '{model_path}'를 찾을 수 없습니다. 현재 모델 상태로 테스트를 진행합니다.\")\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "true_ratings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for user_ids, business_ids, sentiment_vectors, stars in test_loader:\n",
    "        predictions = model(user_ids, business_ids, sentiment_vectors)\n",
    "        test_predictions.extend(predictions.tolist())\n",
    "        true_ratings.extend(stars.tolist())\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(true_ratings, test_predictions))\n",
    "print(f\"\\n최종 테스트 RMSE (최적 모델 기준): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "189f0817-3509-4d07-bc2a-1451053cf079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 수: 447796\n",
      "학습 데이터 수: 313456 (70.00%)\n",
      "검증 데이터 수: 44780 (10.00%)\n",
      "테스트 데이터 수: 89560 (20.00%)\n",
      "\n",
      "UBCF 모델 학습 시작...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "UBCF 모델 학습 완료.\n",
      "UBCF 모델 예측 시작...\n",
      "UBCF 모델 예측 완료.\n",
      "\n",
      "UBCF (n=100) 테스트 RMSE: 1.1139\n",
      "\n",
      "IBCF 모델 학습 시작...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "IBCF 모델 학습 완료.\n",
      "IBCF 모델 예측 시작...\n",
      "IBCF 모델 예측 완료.\n",
      "\n",
      "IBCF (n=100) 테스트 RMSE: 1.1788\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from surprise import Reader, Dataset, KNNBasic\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "from surprise import accuracy # RMSE 계산을 위함\n",
    "\n",
    "# JSONL 파일 로드\n",
    "# 파일 경로를 실제 파일 경로로 바꿔주세요.\n",
    "df = pd.read_json('review_business_5up_5aspect_3sentiment_vectorized_clean.json', lines=True)\n",
    "\n",
    "# 필요한 컬럼 추출 및 SettingWithCopyWarning 해결\n",
    "df_processed = df[['user_id', 'business_id', 'stars', 'sentiment_vector']].copy()\n",
    "\n",
    "# user_id와 business_id를 연속적인 정수 ID로 인코딩 (AAT-Rec과의 일관성을 위해 유지)\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "\n",
    "# 데이터 분할 (7:1:2 비율)\n",
    "train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "val_size_ratio = 1 / 8\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size_ratio, random_state=42)\n",
    "\n",
    "print(f\"전체 데이터 수: {len(df_processed)}\")\n",
    "print(f\"학습 데이터 수: {len(train_df)} ({len(train_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"검증 데이터 수: {len(val_df)} ({len(val_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"테스트 데이터 수: {len(test_df)} ({len(test_df)/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "# UBCF 모델 구현 및 평가\n",
    "# Surprise 라이브러리를 위한 데이터 로드\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# 학습 데이터 (train_df)로 train_set 구성\n",
    "train_set_for_surprise = train_df[['user_id', 'business_id', 'stars']].copy()\n",
    "data_for_train = Dataset.load_from_df(train_set_for_surprise, reader)\n",
    "train_set = data_for_train.build_full_trainset()\n",
    "\n",
    "# 테스트 데이터 (test_df)로 test_set 구성\n",
    "test_set = [(row['user_id'], row['business_id'], row['stars']) for _, row in test_df.iterrows()]\n",
    "\n",
    "# UBCF (User-Based Collaborative Filtering) 모델 정의\n",
    "ubcf_model = KNNBasic(sim_options={'name': 'cosine', 'user_based': True}, k=100)\n",
    "\n",
    "print(\"\\nUBCF 모델 학습 시작...\")\n",
    "ubcf_model.fit(train_set)\n",
    "print(\"UBCF 모델 학습 완료.\")\n",
    "\n",
    "print(\"UBCF 모델 예측 시작...\")\n",
    "predictions_ubcf = ubcf_model.test(test_set)\n",
    "print(\"UBCF 모델 예측 완료.\")\n",
    "\n",
    "rmse_ubcf = accuracy.rmse(predictions_ubcf, verbose=False)\n",
    "print(f\"\\nUBCF (n=100) 테스트 RMSE: {rmse_ubcf:.4f}\")\n",
    "\n",
    "\n",
    "# IBCF (Item-Based Collaborative Filtering) 구현 및 평가\n",
    "ibcf_model = KNNBasic(sim_options={'name': 'cosine', 'user_based': False}, k=100)\n",
    "\n",
    "print(\"\\nIBCF 모델 학습 시작...\")\n",
    "ibcf_model.fit(train_set) # UBCF와 동일한 train_set 사용\n",
    "print(\"IBCF 모델 학습 완료.\")\n",
    "\n",
    "print(\"IBCF 모델 예측 시작...\")\n",
    "predictions_ibcf = ibcf_model.test(test_set) # UBCF와 동일한 test_set 사용\n",
    "print(\"IBCF 모델 예측 완료.\")\n",
    "\n",
    "rmse_ibcf = accuracy.rmse(predictions_ibcf, verbose=False)\n",
    "print(f\"\\nIBCF (n=100) 테스트 RMSE: {rmse_ibcf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94678aa2-38b3-4415-9fe3-90aec3e202f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
